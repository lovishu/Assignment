{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPXl4XidwJQ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.  What is a Decision Tree, and how does it work\n",
        "\n",
        "Ans1.\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It mimics human decision-making by using a tree-like model of decisions and their possible consequences.\n",
        "\n",
        "| Age   | Income | Buys Product |\n",
        "| ----- | ------ | ------------ |\n",
        "| <30   | High   | No           |\n",
        "| 30‚Äì40 | Medium | Yes          |\n",
        "| >40   | Low    | Yes          |\n",
        "\n",
        "\n",
        "Advantages\n",
        "Easy to interpret and visualize\n",
        "\n",
        "Handles both numerical and categorical data\n",
        "\n",
        "\n",
        "\n",
        "Q2.  What are impurity measures in Decision Trees\n",
        "\n",
        "Ans2.Impurity measures in Decision Trees are metrics used to quantify how mixed or impure the classes are at a node. They help the tree decide where to split the data at each step. A lower impurity means a better (purer) split.\n",
        "\n",
        "üîπ Common Impurity Measures (for Classification)\n",
        "1. Gini Impurity\n",
        "Measures the probability of incorrectly classifying a randomly chosen element.\n",
        "\n",
        "2. Entropy (Information Gain)\n",
        "Based on information theory; measures the amount of uncertainty or disorder.\n",
        "\n",
        "3. Mean Squared Error (MSE)\n",
        "Measures average squared difference between actual and predicted values.\n",
        "\n",
        "\n",
        "Q3.  What is the mathematical formula for Gini Impurity\n",
        "\n",
        "Ans3.The mathematical formula for Gini Impurity at a node is:\n",
        "\n",
        "Gini=1‚àí\n",
        "‚àë\n",
        "Cp\n",
        "The Gini Impurity measures the chance of misclassification if a random sample is labeled according to the class distribution at that node.\n",
        "\n",
        "\n",
        "\n",
        "Q4.  What is the mathematical formula for Entropy\n",
        "\n",
        "ANs4.\n",
        "\n",
        "Entropy=‚àí\n",
        "‚àëp\n",
        "log\n",
        "(p\n",
        "\n",
        "\n",
        "Q5.  What is Information Gain, and how is it used in Decision Trees\n",
        "\n",
        "Ans5.Information Gain (IG) is a metric used in Decision Trees (especially in the ID3 and C4.5 algorithms) to select the best feature to split the data at each node.\n",
        "\n",
        "\n",
        "\n",
        "| Term             | Meaning                                              |\n",
        "| ---------------- | ---------------------------------------------------- |\n",
        "| Entropy          | Measures disorder in a dataset                       |\n",
        "| Information Gain | Measures the reduction in disorder after a split     |\n",
        "| Used in          | ID3, C4.5 algorithms for feature selection           |\n",
        "| Goal             | Maximize IG to create pure (low-entropy) child nodes |\n",
        "\n",
        "\n",
        "Q6. What is the difference between Gini Impurity and Entropy\n",
        "\n",
        "Ans6.The difference between Gini Impurity and Entropy lies in how they measure impurity and influence the decision tree's split decisions. Both are used for classification, but have different mathematical formulations and behaviors.\n",
        "\n",
        "| Metric            | Formula                                     |\n",
        "| ----------------- | ------------------------------------------- |\n",
        "| **Gini Impurity** | $Gini = 1 - \\sum_{i=1}^{C} p_i^2$           |\n",
        "| **Entropy**       | $Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$ |\n",
        "\n",
        "\n",
        "Q7. What is the mathematical explanation behind Decision Trees\n",
        "\n",
        "Ans7.The mathematical explanation behind Decision Trees is based on recursive partitioning of the feature space using an impurity measure (e.g., Gini or Entropy) to choose the best splits at each node. The goal is to build a tree that maps inputs to outputs by minimizing impurity at each step.\n",
        "\n",
        "üîπ 1. Basic Structure\n",
        "\n",
        "Nodes: Represent features/conditions\n",
        "\n",
        "Edges: Represent outcomes of those conditions\n",
        "\n",
        "Leaves: Represent final predictions (class or value)\n",
        "\n",
        "\n",
        "\n",
        "üîπ 2. Mathematical Process (Classification Case)\n",
        "Step 1: Choose the Best Feature to Split\n",
        "Step 2: Split the Dataset\n",
        "Step 3: Stopping Criteria\n",
        "\n",
        "\n",
        "Q8. What is Pre-Pruning in Decision Trees\n",
        "\n",
        "Ans8. Pre-Pruning (also called early stopping) is a technique used in Decision Trees to prevent overfitting by stopping the tree growth early, before it becomes too complex.\n",
        "\n",
        "Pre-pruning aims to simplify the model and improve generalization by limiting the size of the tree during training.\n",
        "\n",
        "| Criterion                     | Description                                                                                           |\n",
        "| ----------------------------- | ----------------------------------------------------------------------------------------------------- |\n",
        "| **Maximum Depth**             | Stop if the tree reaches a certain depth (`max_depth`)                                                |\n",
        "| **Minimum Samples per Node**  | Stop if a node has fewer than a certain number of samples (`min_samples_split` or `min_samples_leaf`) |\n",
        "| **Maximum Number of Nodes**   | Stop if total number of nodes exceeds a threshold                                                     |\n",
        "| **Minimum Impurity Decrease** | Stop if the reduction in impurity from a split is below a threshold (`min_impurity_decrease`)         |\n",
        "| **No Gain**                   | Stop if no feature provides useful information (e.g., gain = 0)                                       |\n",
        "\n",
        "\n",
        "\n",
        "Q9. What is Post-Pruning in Decision Trees\n",
        "\n",
        "Ans9.‚úÖ Benefits of Pre-Pruning\n",
        "Reduces overfitting\n",
        "\n",
        "Speeds up training\n",
        "\n",
        "\n",
        "| Aspect       | Pre-Pruning              | Post-Pruning                     |\n",
        "| ------------ | ------------------------ | -------------------------------- |\n",
        "| When Applied | During tree growth       | After full tree is built         |\n",
        "| Risk         | Underfitting             | Less risk if done properly       |\n",
        "| Control      | Uses stopping conditions | Uses subtree replacement/cutting |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q10. What is the difference between Pre-Pruning and Post-Pruning\n",
        "\n",
        "Ans10.\n",
        "\n",
        "| Feature                  | **Pre-Pruning**                                           | **Post-Pruning**                            |\n",
        "| ------------------------ | --------------------------------------------------------- | ------------------------------------------- |\n",
        "| **When applied**         | During tree construction                                  | After full tree is built                    |\n",
        "| **Control method**       | Stopping criteria (e.g., max depth, min samples)          | Error-based pruning or complexity reduction |\n",
        "| **Risk of underfitting** | Higher (may stop too early)                               | Lower (tree fully explores all paths first) |\n",
        "| **Complexity**           | Simpler, faster to train                                  | More accurate but computationally intensive |\n",
        "| **Common parameters**    | `max_depth`, `min_samples_split`, `min_impurity_decrease` | `ccp_alpha`, reduced error pruning          |\n",
        "| **Implementation**       | Integrated into tree-growing loop                         | Requires extra pass after training          |\n",
        "\n",
        "\n",
        "\n",
        "Q11.  What is a Decision Tree Regressor\n",
        "\n",
        "\n",
        "Ans11. A Decision Tree Regressor is a type of decision tree algorithm used for regression tasks, where the goal is to predict a continuous numerical value, rather than a class label.\n",
        "\n",
        "üî∏ Key Idea:\n",
        "A Decision Tree Regressor partitions the feature space into regions and assigns each region a predicted value, typically the mean of the target values in that region.\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "reg = DecisionTreeRegressor(max_depth=3)\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "Q12.  What are the advantages and disadvantages of Decision Trees\n",
        "\n",
        "Ans12.\n",
        "\n",
        "Here are the advantages and disadvantages of Decision Trees:\n",
        "\n",
        "‚úÖ Advantages\n",
        "Easy to understand and interpret\n",
        "\n",
        "Handles both numerical and categorical data\n",
        "\n",
        "Requires little data preparation\n",
        "\n",
        "Can handle multi-output problems\n",
        "\n",
        "‚ùå Disadvantages\n",
        "Prone to overfitting\n",
        "\n",
        "Unstable to small data variations\n",
        "\n",
        "Bias towards features with more levels\n",
        "\n",
        "\n",
        "\n",
        "Q13.  How does a Decision Tree handle missing values\n",
        "\n",
        "Ans13.Decision Trees can handle missing values using several strategies, depending on the implementation. Here are the common approaches:\n",
        "\n",
        "1. Surrogate Splits (Used in CART and some libraries)\n",
        "When the primary splitting feature has a missing value for a sample, the tree looks for surrogate features ‚Äî other features highly correlated with the primary feature.\n",
        "\n",
        "2. Assign Missing Values to the Most Frequent Branch\n",
        "For samples with missing values at a split, assign them to the child node (left or right) that has the majority of training samples.\n",
        "\n",
        "3. Use Probabilistic Splitting\n",
        "Instead of assigning the sample to a single branch, distribute it probabilistically to child nodes based on the proportion of training samples.\n",
        "\n",
        "\n",
        "\n",
        "Q14.  How does a Decision Tree handle categorical features\n",
        "\n",
        "\n",
        "Ans14. A Decision Tree handles categorical features by splitting the data based on the categories, but the exact approach depends on whether the feature is nominal (unordered categories) or ordinal (ordered categories) and on the implementation. Here's how it typically works:\n",
        "\n",
        "\n",
        "1. Splitting on Categorical Features\n",
        "2. Handling Nominal vs Ordinal Features\n",
        "Nominal (unordered):\n",
        "Categories are treated as distinct groups without any order. Splits are based on grouping categories.\n",
        "\n",
        "Ordinal (ordered):\n",
        "Categories have a natural order (e.g., Low < Medium < High). The tree can treat these as numerical and find threshold splits like feature <= Medium.\n",
        "\n",
        "\n",
        "\n",
        "3. Encoding Before Splitting\n",
        "Some implementations internally convert categorical features into numerical codes (e.g., label encoding), then find splits on those codes.\n",
        "\n",
        "\n",
        "| Method                         | Description                                 | Typical Use               |\n",
        "| ------------------------------ | ------------------------------------------- | ------------------------- |\n",
        "| Binary Splitting on Categories | Group categories into two subsets per split | CART (scikit-learn)       |\n",
        "| Multiway Splitting             | One branch per category                     | C4.5, some others         |\n",
        "| Ordinal Treatment              | Treat ordered categories as numerical       | If order is known         |\n",
        "| Encoding Before Splitting      | Convert categories to numbers for splits    | Sometimes used, but risky |\n",
        "\n",
        "\n",
        "\n",
        "Q15. What are some real-world applications of Decision Trees?\n",
        "\n",
        "\n",
        "Ans15.1. Healthcare\n",
        "Diagnosing diseases based on symptoms and patient data\n",
        "\n",
        "Predicting patient outcomes or risk factors\n",
        "\n",
        "2. Finance\n",
        "Credit scoring and loan approval\n",
        "\n",
        "Fraud detection in transactions\n",
        "3. Marketing\n",
        "Customer segmentation\n",
        "\n",
        "Predicting customer behavior and purchase intent\n",
        "\n",
        "4. Retail\n",
        "Inventory management based on sales patterns\n",
        "\n",
        "\n",
        "\n",
        "Q16.  Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
        "\n",
        "Ans16.from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(_\n",
        "\n",
        "\n",
        "Q17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances\n",
        "\n",
        "\n",
        "Ans17.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy using Gini Impurity: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "Q18.  Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the Model Accuracy\n",
        "Ans18.from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier with Entropy criterion\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy using Entropy: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "Q19.  Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error\n",
        "\n",
        "Ans19.from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier with Entropy criterion\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy using Entropy: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "Q20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "\n",
        "\n",
        "Ans20.from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data (train/test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Export tree in DOT format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Visualize the tree using graphviz\n",
        "grap\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NyUm4mRmwNGU"
      }
    }
  ]
}