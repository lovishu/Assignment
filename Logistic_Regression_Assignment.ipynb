{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTU4aALlfS9P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.  What is Logistic Regression, and how does it differ from Linear Regression\n",
        "\n",
        "Ana1. Logistic Regression and Linear Regression are both supervised learning algorithms used for different types of predictive tasks:\n",
        "\n",
        "üîπ Logistic Regression\n",
        "Purpose: Used for classification tasks (especially binary classification).\n",
        "\n",
        "Output: Predicts the probability of a class label (e.g., 0 or 1).\n",
        "\n",
        "Function: Applies the logistic (sigmoid) function to a linear combination of inputs to squash output to a range between 0 and 1.\n",
        "\n",
        "Example Use Case: Spam detection (email is spam or not), disease prediction (positive/negative).\n",
        "\n",
        "üîπ Linear Regression\n",
        "Purpose: Used for regression tasks (predicting continuous values).\n",
        "\n",
        "Output: Predicts a real-numbered output (e.g., salary, house price).\n",
        "\n",
        "Function: Computes a linear combination of the input features.\n",
        "\n",
        "\n",
        "\n",
        "Q2. What is the mathematical equation of Logistic Regression\n",
        "\n",
        "Ans 2. The mathematical equation of Logistic Regression involves two main parts:\n",
        "\n",
        "üîπ 1. Linear Combination of Inputs\n",
        "Just like linear regression, logistic regression starts by computing a linear combination of the input features:\n",
        "\n",
        "üîπ 2. Sigmoid (Logistic) Function\n",
        "To convert the output into a probability between 0 and 1, we apply the sigmoid function:\n",
        "\n",
        "\n",
        "\n",
        "Q3.  Why do we use the Sigmoid function in Logistic Regression\n",
        "\n",
        "Ans3. We use the sigmoid function in logistic regression because it converts any real-valued number into a value between 0 and 1, making it ideal for probability estimation in classification problems.\n",
        "\n",
        "‚úÖ Main Reasons for Using Sigmoid\n",
        "1. Probability Output\tThe sigmoid squashes the linear output to a [0, 1] range, allowing interpretation as a probability.\n",
        "\n",
        "2. Binary Classification\tIn logistic regression, we model the probability of a binary outcome (e.g., success/failure, 0/1).\n",
        "\n",
        "The sigmoid is smooth and has a well-defined derivative, which is essential for optimization using gradient descent.\n",
        "\n",
        "\n",
        "Q4.  What is the cost function of Logistic Regression\n",
        "\n",
        "Ans4. The cost function of Logistic Regression is based on the log loss or binary cross-entropy function, which measures how well the model's predicted probabilities match the actual binary labels.\n",
        "\n",
        "üîπ 1. Hypothesis (Prediction)\n",
        "\n",
        "Logistic regression predicts the probability that a given input belongs to class 1:\n",
        "\n",
        "üîπ 3. Overall Cost Function (Log Loss)\n",
        "m training examples:\n",
        "\n",
        "üîπ 4. Why This Cost Function?\n",
        "It‚Äôs convex, allowing reliable optimization via gradient descent.\n",
        "\n",
        "It penalizes wrong predictions heavily, especially confident incorrect ones (e.g., predicting 0.99 when the label is 0).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q5. What is Regularization in Logistic Regression? Why is it needed\n",
        "\n",
        "Ans5. Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function of logistic regression. This penalty discourages the model from fitting the training data too closely by shrinking the coefficients (weights) toward zero.\n",
        "\n",
        "Why is Regularization Needed?\n",
        "\n",
        "\n",
        "Overfitting occurs when the model learns noise or random fluctuations in the training data, resulting in poor performance on unseen data.\n",
        "\n",
        "Regularization helps the model to generalize better by controlling the complexity of the model.\n",
        "\n",
        "It prevents large weights that could cause the model to be overly sensitive to input features.\n",
        "\n",
        "Types of Regularization in Logistic Regression:\n",
        "\n",
        "\n",
        "\n",
        "Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression\n",
        "\n",
        "Ans6. Here‚Äôs a precise comparison of Lasso, Ridge, and Elastic Net regression, which are regularization techniques used to prevent overfitting in linear models (including logistic regression):\n",
        "\n",
        "| Feature                    | Lasso Regression (L1)                                                      | Ridge Regression (L2)                                        | Elastic Net Regression                                         |                                                                           |                                            |      |                           |\n",
        "| -------------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------ | -------------------------------------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------ | ---- | ------------------------- |\n",
        "| **Penalty Term**           | Sum of absolute values of coefficients:            (\\lambda \\sum           | w\\_j                                                         | )                                                              | Sum of squared coefficients:                         $\\lambda \\sum w_j^2$ | Combination of L1 and L2: (\\lambda\\_1 \\sum | w\\_j | + \\lambda\\_2 \\sum w\\_j^2) |\n",
        "| **Effect on Coefficients** | Can shrink some coefficients exactly to zero ‚Üí **feature selection**       | Shrinks coefficients towards zero but **never exactly zero** | Encourages sparsity like Lasso and smooth shrinkage like Ridge |                                                                           |                                            |      |                           |\n",
        "| **When to Use**            | When you want a **sparse model** (select important features)               | When you want to keep **all features but reduce magnitude**  | When you want a balance: some sparsity + grouping effect       |                                                                           |                                            |      |                           |\n",
        "| **Geometry of Penalty**    | Diamond-shaped constraint (promotes zeros)                                 | Circular/elliptical constraint (smooth shrinkage)            | Combination of both shapes                                     |                                                                           |                                            |      |                           |\n",
        "| **Computationally**        | Can be more computationally expensive due to non-differentiability at zero | Computationally simpler due to differentiability             | More complex, but flexible                                     |                                                                           |                                            |      |                           |\n",
        "\n",
        "\n",
        "\n",
        "Q6.When should we use Elastic Net instead of Lasso or Ridge\n",
        "\n",
        "Ans6. You should use Elastic Net instead of Lasso or Ridge regression when:\n",
        "\n",
        "1. Features are Highly Correlated\n",
        "When your dataset has groups of correlated features, Lasso tends to select only one feature from the group and ignore others.\n",
        "\n",
        "Elastic Net can select multiple correlated features together, combining the strengths of both L1 (sparsity) and L2 (grouping) penalties.\n",
        "\n",
        "2. Need a Balance Between Sparsity and Shrinkage\n",
        "If you want a model that is sparse (like Lasso) but also wants to shrink coefficients smoothly (like Ridge), Elastic Net offers this trade-off by combining both penalties.\n",
        "\n",
        "3. When Lasso Fails or is Too Aggressive\n",
        "\n",
        "4. High-Dimensional Data with Many Features\n",
        "\n",
        "\n",
        "\n",
        "Q8.  What is the impact of the regularization parameter (Œª) in Logistic Regression\n",
        "\n",
        "Ans8.\n",
        "Œª in logistic regression controls the strength of the regularization penalty applied to the model‚Äôs weights. Its impact is crucial for balancing model complexity and generalization.\n",
        "\n",
        "More Details:\n",
        " (for L2) is almost zero, so the model focuses mainly on minimizing the training loss, possibly capturing noise.\n",
        "\n",
        "Œª: The penalty dominates, forcing weights close to zero, reducing model complexity but risking underfitting by ignoring important features.\n",
        "\n",
        "Œª: Found via techniques like cross-validation to strike the best trade-off between bias and variance.\n",
        "\n",
        "\n",
        "\n",
        "Q9. What are the key assumptions of Logistic Regression\n",
        "\n",
        "Ans9.Here are the key assumptions of Logistic Regression:\n",
        "\n",
        "1. Binary Outcome Variable\n",
        "The dependent variable is binary (takes values 0 or 1).\n",
        "\n",
        "2. Linear Relationship between Log-Odds and Predictors\n",
        "The log-odds (logit) of the outcome is a linear combination of the input variables:\n",
        "\n",
        "5. Large Sample Size\n",
        "Logistic regression relies on maximum likelihood estimation, which performs best with a reasonably large dataset.\n",
        "\n",
        "Small samples can lead to unreliable estimates.\n",
        "\n",
        "6. No Extreme Outliers Influencing the Model\n",
        "\n",
        "Outliers in predictor variables can disproportionately affect the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q9.  What are some alternatives to Logistic Regression for classification tasks\n",
        "\n",
        "Ans9.1. Decision Trees\n",
        "Non-linear model that splits data based on feature thresholds.\n",
        "\n",
        "Easy to interpret, handles categorical and numerical data.\n",
        "\n",
        "\n",
        "Can capture complex relationships without requiring linearity.\n",
        "\n",
        "2. Random Forest\n",
        "Ensemble of decision trees using bagging.\n",
        "\n",
        "Reduces overfitting compared to a single tree.\n",
        "\n",
        "Good accuracy and robustness.\n",
        "\n",
        "3. Support Vector Machines (SVM)\n",
        "Finds the optimal hyperplane to separate classes.\n",
        "\n",
        "4. K-Nearest Neighbors (KNN)\n",
        "Classifies based on majority vote of neighbors.\n",
        "\n",
        "\n",
        "\n",
        "Q11.  What are Classification Evaluation Metrics\n",
        "\n",
        "\n",
        "Ans11.Classification evaluation metrics are used to measure how well a classification model performs. Here are the key metrics commonly used:\n",
        "\n",
        "1. Accuracy\n",
        "Definition: Proportion of correctly predicted instances (both positive and negative) over total instances.\n",
        "\n",
        "Use: Good when classes are balanced.\n",
        "\n",
        "2. Precision\n",
        "Definition: Proportion of correctly predicted positive instances out of all predicted positives.\n",
        "\n",
        "\n",
        "3. Recall (Sensitivity or True Positive Rate)\n",
        "\n",
        "\n",
        "4. F1 Score\n",
        "\n",
        "5. Specificity (True Negative Rate)\n",
        "\n",
        "\n",
        "Q12.  How does class imbalance affect Logistic Regression\n",
        "Ans12  Class imbalance can significantly affect the performance of Logistic Regression in the following ways:\n",
        "\n",
        "1. Bias Toward Majority Class\n",
        "\n",
        "2. Poor Probability Estimates\n",
        "\n",
        "3. Misleading Accuracy\n",
        "\n",
        "How to Handle Class Imbalance in Logistic Regression?\n",
        "\n",
        "Resampling Techniques:\n",
        "\n",
        "Oversampling the minority class (e.g., SMOTE)\n",
        "\n",
        "Use of Class Weights:\n",
        "\n",
        "Assign higher penalty to misclassifying minority class by setting class weights in logistic regression.\n",
        "\n",
        "\n",
        "Threshold Adjustment:\n",
        "Change the classification threshold from 0.5 to a value favoring minority class detection.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q13. What is Hyperparameter Tuning in Logistic Regression\n",
        "\n",
        "Ans13. Hyperparameter tuning in logistic regression is the process of selecting the best values for parameters that control the training process but are not learned directly from the data.\n",
        "\n",
        "Key Hyperparameters in Logistic Regression:\n",
        "Regularization parameter (Œª or C):\n",
        "\n",
        "Controls the strength of regularization.\n",
        "\n",
        "Œª is the regularization strength (larger means stronger regularization).\n",
        "In many libraries like scikit-learn, C = 1 /\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q14. What are different solvers in Logistic Regression? Which one should be used\n",
        "\n",
        "Ans14. In logistic regression, solvers are optimization algorithms used to minimize the cost function (often with regularization). Different solvers are suitable for different scenarios depending on dataset size, regularization type, and whether you're using L1 or L2 penalties.\n",
        "\n",
        "üîß Common Solvers in Logistic Regression (e.g., in scikit-learn):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "| Solver        | Type                                     | Supports L1 | Supports L2 | Use Case                                                                                |\n",
        "| ------------- | ---------------------------------------- | ----------- | ----------- | --------------------------------------------------------------------------------------- |\n",
        "| **liblinear** | Coordinate descent (good for small data) | ‚úÖ Yes       | ‚úÖ Yes       | Binary/multiclass (one-vs-rest); **supports L1**; slower on large data                  |\n",
        "| **lbfgs**     | Quasi-Newton (BFGS variant)              | ‚ùå No        | ‚úÖ Yes       | Efficient for **large datasets** and multiclass problems                                |\n",
        "| **sag**       | Stochastic Average Gradient              | ‚ùå No        | ‚úÖ Yes       | Large datasets, **requires feature scaling**                                            |\n",
        "| **saga**      | Stochastic Average Gradient Descent      | ‚úÖ Yes       | ‚úÖ Yes       | Large-scale datasets; supports both L1 & L2; faster than liblinear on large sparse data |\n",
        "| **newton-cg** | Newton's method (approximate Hessian)    | ‚ùå No        | ‚úÖ Yes       | Multiclass problems, L2 regularization only                                             |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q15.  How is Logistic Regression extended for multiclass classification\n",
        "\n",
        "Ans15. Logistic Regression is naturally a binary classifier, but it can be extended for multiclass classification using the following strategies:\n",
        "\n",
        "1. One-vs-Rest (OvR or One-vs-All) ‚Äì Default in scikit-learn\n",
        "For K classes, train K binary classifiers: each one predicts whether an observation belongs to that class or not.\n",
        "\n",
        "At prediction time, choose the class with the highest probability output by the classifiers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q15.  What are the advantages and disadvantages of Logistic Regression\n",
        "\n",
        "Ans15. Here are the advantages and disadvantages of Logistic Regression:\n",
        "\n",
        "‚úÖ Advantages:\n",
        "Simple and Interpretable\n",
        "\n",
        "Easy to understand how features influence the prediction (via model coefficients).\n",
        "\n",
        "Coefficients indicate the direction and strength of influence on the log-odds.\n",
        "\n",
        "Fast and Efficient\n",
        "\n",
        "Computationally inexpensive and quick to train, even on large datasets.\n",
        "\n",
        "Works Well with Linearly Separable Data\n",
        "\n",
        "\n",
        "\n",
        "Q17.  What are some use cases of Logistic Regression\n",
        "\n",
        "Ans 17. Here are some common use cases of Logistic Regression, across industries and domains:\n",
        "\n",
        "üîê 1. Binary Classification Tasks\n",
        "Spam Detection: Classify emails as spam or not spam.\n",
        "\n",
        "Fraud Detection: Flag transactions as fraudulent or legitimate.\n",
        "\n",
        "üè• 2. Healthcare and Medical Diagnosis\n",
        "\n",
        "üë®‚Äçüíº 3. Marketing and Customer Analytics\n",
        "\n",
        "üèõ 4. Social Science and Surveys\n",
        "\n",
        "\n",
        "\n",
        "Q18.  What is the difference between Softmax Regression and Logistic Regression\n",
        "\n",
        "An18. The key difference between Softmax Regression and Logistic Regression lies in the number of classes they are designed to handle and how they compute class probabilities.\n",
        "\n",
        "\n",
        "üîπ 1. Logistic Regression\n",
        "Used for: Binary classification (2 classes)\n",
        "\n",
        "üîπ 2. Softmax Regression (also called Multinomial Logistic Regression)\n",
        "Used for: Multiclass classification (more than 2 classes)\n",
        "\n",
        "Output: A probability distribution over all classes\n",
        "\n",
        "| Feature             | Logistic Regression   | Softmax Regression              |\n",
        "| ------------------- | --------------------- | ------------------------------- |\n",
        "| Number of Classes   | 2                     | 3 or more                       |\n",
        "| Output              | Single probability    | Vector of probabilities         |\n",
        "| Activation Function | Sigmoid               | Softmax                         |\n",
        "| Decision Rule       | Threshold (e.g., 0.5) | Argmax of probabilities         |\n",
        "| Cost Function       | Binary cross-entropy  | Categorical cross-entropy       |\n",
        "| Use Case Example    | Spam vs Not Spam      | Classify animals: cat, dog, cow |\n",
        "\n",
        "\n",
        "Q19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification\n",
        "\n",
        "Ans19. Choosing between One-vs-Rest (OvR) and Softmax (Multinomial Logistic Regression) depends on your dataset, problem requirements, and computational considerations.\n",
        "\n",
        "| Criteria           | One-vs-Rest (OvR)                            | Softmax (Multinomial)                           |\n",
        "| ------------------ | -------------------------------------------- | ----------------------------------------------- |\n",
        "| Model type         | Trains **K** binary classifiers              | Trains **1 unified** multiclass model           |\n",
        "| Output             | One binary decision per class                | Single probability distribution                 |\n",
        "| Interpretation     | Easier to interpret per class                | More holistic view of all classes               |\n",
        "| Computational cost | Slightly higher (K models)                   | Lower (single model)                            |\n",
        "| When to use        | Classes are **very unbalanced** or separable | Classes are **mutually exclusive** and balanced |\n",
        "\n",
        "Q20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "\n",
        "Ans20. In Logistic Regression, coefficients represent the effect of each feature on the log-odds of the positive class. Here's how to interpret them step-by-step:\n",
        "\n",
        "üî¢ 1. Log-Odds Interpretation\n",
        "\n",
        "üîÅ 2. Odds Ratio Interpretation\n",
        "You can exponentiate the coefficient to get the odds ratio:\n",
        "\n",
        "\n",
        "Odds¬†Ratio=e\n",
        "\n",
        "üìå Example: Suppose you have a logistic regression model predicting whether a person buys a product based on income:\n",
        "\n",
        "\n",
        "\n",
        "üß† Important Notes:\n",
        "Sign of coefficient:\n",
        "\n",
        "\n",
        "Positive ‚Üí Increases probability of class 1\n",
        "\n",
        "Negative ‚Üí Decreases probability of class 1\n",
        "\n",
        "Coefficients are meaningful only if:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Practical\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q1.  Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n",
        "\n",
        "Ans1.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data       # Features\n",
        "y = iris.target     # Labels\n",
        "\n",
        "# Step 2: Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q2.  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "\n",
        "\n",
        "Ans2.  \n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into train a\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "\n",
        "\n",
        "Ans3.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', C=1.0)\n",
        "model.fit(X_train,_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tGigajzWfXgF"
      }
    }
  ]
}