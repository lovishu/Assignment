{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flp2zegEpe1G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.  Explain the difference between AWS Regions, Availability Zones, and Edge Locations. Why is this important for data analysis and latency-sensitive applications\n",
        "\n",
        "Ans1. Difference Between AWS Regions, Availability Zones, and Edge Locations\n",
        "1. AWS Regions\n",
        "Definition:\n",
        "A Region is a geographically distinct location where AWS has data centers. Each Region contains multiple Availability Zones.\n",
        "\n",
        "us-east-1 (N. Virginia), ap-south-1 (Mumbai), eu-west-1 (Ireland).\n",
        "\n",
        "2. Availability Zones (AZs)\n",
        "| Aspect                          | Explanation                                                           |\n",
        "| ------------------------------- | --------------------------------------------------------------------- |\n",
        "| **Latency**                     | Edge Locations reduce latency by serving data closer to users.        |\n",
        "| **High Availability**           | Multiple AZs provide fault tolerance by isolating failures.           |\n",
        "| **Data Residency & Compliance** | Regions allow you to keep data within specific geographic boundaries. |\n",
        "| **Performance**                 | Selecting the right Region and AZ optimizes speed and reliability.    |\n",
        "| **Disaster Recovery**           | AZs and Regions allow for backup and failover strategies.             |\n",
        "\n",
        "\n",
        "\n",
        "Q2. Using the AWS CLI, list all available AWS regions. Share the command used and the output\n",
        "\n",
        "Ans2.\n",
        "aws ec2 describe-regions --query \"Regions[].RegionName\" --output text\n",
        "Explanation:\n",
        "aws ec2 describe-regions: Fetches all AWS regions.\n",
        "\n",
        "--query \"Regions[].RegionName\": Filters the output to show only the region names.\n",
        "--output text: Displays the output as plain text for easier reading.\n",
        "\n",
        "us-east-1 us-east-2 us-west-1 us-west-2 af-south-1 ap-east-1 ap-south-1 ap-northeast-1 ap-northeast-2 ap-northeast-3 ap-southeast-1 ap-southeast-2 ca-central-1 eu-central-1 eu-west-1 eu-west-2 eu-west-3 eu-north-1 eu-south-1 me-south-1 sa-east-1\n",
        "\n",
        "\n",
        "Q3.  Create a new IAM user with least privilege access to Amazon S3. Share your attached policies (JSON or\n",
        "\n",
        "Ans3.To create a new IAM user with least privilege access to Amazon S3, you typically grant only the necessary permissions to perform specific S3 actions. For example, read-only access to S3 buckets or limited write access to certain buckets.\n",
        "\n",
        "Example: IAM Policy for Least Privilege Access to S3 (Read-Only)\n",
        "{\n",
        "    \"Version\": \"2012-10-17\",\n",
        "    \"Statement\": [\n",
        "        {\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"s3:GetObject\",\n",
        "                \"s3:ListBucket\"\n",
        "            ],\n",
        "            \"Resource\": [\n",
        "                \"arn:aws:s3:::example-bucket\",\n",
        "                \"arn:aws:s3:::example-bucket/*\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "Q4.  Compare different Amazon S3 storage (Standard, Intelligent-Tiering, Glacier). When should each be used in data analytics workflows\n",
        "\n",
        "\n",
        "Ans4.\n",
        "\n",
        "| Storage Class              | Description                                                                                    | Use Case in Data Analytics Workflows                                                                                                                          | Cost & Access Characteristics                                      |\n",
        "| -------------------------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| **S3 Standard**            | High durability, availability, and low latency. Designed for frequently accessed data.         | Use for active datasets, real-time analytics, and frequently queried data. Ideal for storing raw data, intermediate results, or data that needs quick access. | Higher cost, immediate access, no retrieval delay.                 |\n",
        "| **S3 Intelligent-Tiering** | Automatically moves data between frequent and infrequent access tiers based on usage patterns. | Best when access patterns are unknown or unpredictable. Useful for datasets with varying or changing access frequency in analytics pipelines.                 | Slightly higher cost than Standard; saves cost by auto-optimizing. |\n",
        "| **S3 Glacier**             | Low-cost archival storage with retrieval times from minutes to hours.                          | Use for long-term data archiving, historical data, or backups that are rarely accessed but must be retained for compliance or future analysis.                | Very low cost, but retrieval has latency and possible extra fees.  |\n",
        "\n",
        "\n",
        "\n",
        "Q5.  Create an S3 bucket and upload a sample dataset (CSV or JSON). Enable versioning and show at least two  Create an S3 bucket and upload a sample dataset (CSV or JSON). Enable versioning and show at least two\n",
        "\n",
        "Ans5. Here’s a step-by-step guide to create an S3 bucket, upload a sample dataset with versioning enabled, and demonstrate at least two versions of a file using AWS CLI.\n",
        "\n",
        "aws s3api create-bucket --bucket my-sample-bucket-12345 --region us-east-1\n",
        "aws s3api put-bucket-versioning --bucket my-sample-bucket-12345 --versioning-configuration Status=Enabled\n",
        "\n",
        "Step 3: Prepare a Sample Dataset (e.g., data.csv)\n",
        "id,name,age\n",
        "1,Alice,30\n",
        "2,Bob,25\n",
        "\n",
        "\n",
        "\n",
        "{\n",
        "    \"Versions\": [\n",
        "        {\n",
        "            \"ETag\": \"\\\"etagvalue2\\\"\",\n",
        "            \"VersionId\": \"version-id-2\",\n",
        "            \"IsLatest\": true,\n",
        "            \"Key\": \"data.csv\",\n",
        "            \"LastModified\": \"2025-05-28T10:00:00.000Z\",\n",
        "            \"Size\": 56,\n",
        "            \"StorageClass\": \"STANDARD\",\n",
        "            \"Owner\": { ... }\n",
        "        },\n",
        "        {\n",
        "            \"ETag\": \"\\\"etagvalue1\\\"\",\n",
        "            \"VersionId\": \"version-id-1\",\n",
        "            \"IsLatest\": false,\n",
        "            \"Key\": \"data.csv\",\n",
        "            \"LastModified\": \"2025-05-28T09:30:00.000Z\",\n",
        "            \"Size\": 45,\n",
        "            \"StorageClass\": \"STANDARD\",\n",
        "            \"Owner\": { ... }\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "Q6.  Write and apply a lifecycle policy to move files to Glacier after 30 days and delete them after 90. Share the\n",
        "\n",
        "Ans6. Here is a sample S3 lifecycle policy in JSON to:\n",
        "\n",
        "Move objects to Glacier after 30 days\n",
        "\n",
        "Delete objects after 90 days\n",
        "\n",
        "{\n",
        "  \"Rules\": [\n",
        "    {\n",
        "      \"ID\": \"MoveToGlacierAndDelete\",\n",
        "      \"Status\": \"Enabled\",\n",
        "      \"Filter\": {\n",
        "        \"Prefix\": \"\"\n",
        "      },\n",
        "      \"Transitions\": [\n",
        "        {\n",
        "          \"Days\": 30,\n",
        "          \"StorageClass\": \"GLACIER\"\n",
        "        }\n",
        "      ],\n",
        "      \"Expiration\": {\n",
        "        \"Days\": 90\n",
        "      },\n",
        "      \"NoncurrentVersionExpiration\": {\n",
        "        \"NoncurrentDays\": 90\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "\n",
        "Q7.  Compare RDS, DynamoDB, and Redshift for use in different stages of a data pipeline. Give one use case for each\n",
        "\n",
        "Ans7.Here’s a concise comparison of Amazon RDS, DynamoDB, and Redshift focused on their roles in different stages of a data pipeline, along with a use case for each:\n",
        "\n",
        "| Service        | Type                          | Best Used For in Data Pipeline                                                                                                        | Key Features                                                                             | Example Use Case                                                                          |\n",
        "| -------------- | ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
        "| **Amazon RDS** | Relational Database (SQL)     | **Transactional and Operational Data Storage** (OLTP) — Storing structured data, running complex queries, and supporting applications | Managed SQL databases (MySQL, PostgreSQL, etc.), ACID compliance, supports complex joins | Store user profiles and transactional data for real-time app usage                        |\n",
        "| **DynamoDB**   | NoSQL Key-Value / Document DB | **High-Throughput, Low-Latency Data Ingestion & Serving** — Fast reads/writes at scale for semi-structured data                       | Serverless, fully managed, single-digit millisecond latency, scales automatically        | Collect and serve real-time clickstream or IoT data                                       |\n",
        "| **Redshift**   | Data Warehouse (Analytical)   | **Batch Analytics and Reporting** — Large-scale analytical queries on aggregated and historical data                                  | Columnar storage, massively parallel processing, SQL querying optimized for analytics    | Run complex queries and generate business intelligence reports from aggregated sales data |\n",
        "\n",
        "\n",
        "Q8.  Create a DynamoDB table and insert 3 records manually. Then write a Lambda function that adds records when triggered by S3 uploads\n",
        "\n",
        "Ans8.\n",
        "Here’s a step-by-step guide to:\n",
        "\n",
        "Create a DynamoDB table\n",
        "\n",
        "Insert 3 records manually\n",
        "\n",
        "Write an AWS Lambda function that triggers on S3 uploads to add records to the DynamoDB table.\n",
        "\n",
        "aws dynamodb create-table \\\n",
        "  --table-name MyDataTable \\\n",
        "  --attribute-definitions AttributeName=id,AttributeType=S \\\n",
        "  --key-schema AttributeName=id,KeyType=HASH \\\n",
        "  --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n",
        "\n",
        "\n",
        "Q9. What is serverless computing? Discuss pros and cons of using AWS Lambda for data pipelines\n",
        "\n",
        "\n",
        "Ans9. Serverless computing is a cloud computing model where the cloud provider automatically manages the infrastructure, including server provisioning, scaling, and maintenance. Developers focus solely on writing and deploying code without worrying about the underlying servers.\n",
        "\n",
        "In this model, you pay only for the actual compute time your code consumes, rather than for pre-allocated resources.\n",
        "\n",
        "AWS Lambda Overview\n",
        "AWS Lambda is a popular serverless compute service that runs your code in response to events (e.g., file uploads, API calls) and automatically manages the compute resources.\n",
        "\n",
        "\n",
        "| Pros                           | Cons                                |\n",
        "| ------------------------------ | ----------------------------------- |\n",
        "| No server management           | Max 15-minute execution timeout     |\n",
        "| Automatic, seamless scaling    | Cold start latency                  |\n",
        "| Cost-effective pay-per-use     | Limited memory and storage          |\n",
        "| Event-driven integration       | Stateless (need external state)     |\n",
        "| Quick development & deployment | Complexity grows with pipeline size |\n",
        "\n",
        "\n",
        "Q10 Create a Lambda function triggered by S3 uploads that logs file name, size, and timestamp to Cloudwatch.Share code and a log screenshot\n",
        "\n",
        "Ans10.\n",
        "import json\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    # Extract bucket name and object info from the S3 event\n",
        "    record = event['Records'][0]['s3']\n",
        "    bucket = record['bucket']['name']\n",
        "    key = record['object']['key']\n",
        "    size = record['object']['size']\n",
        "    timestamp = record['eventTime']\n",
        "    \n",
        "    # Log details to CloudWatch\n",
        "    logger.info(f\"File uploaded: {key}\")\n",
        "    logger.info(f\"Bucket: {bucket}\")\n",
        "    logger.info(f\"Size (bytes): {size}\")\n",
        "    logger.info(f\"Upload time: {timestamp}\")\n",
        "    \n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': json.dumps('Log recorded successfully')\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "Q11. Use AWS Glue to crawl your S3 dataset, create a Data Catalog table, and run a Glue job to convert CSV data to parquet. Share job code and output location\n",
        "\n",
        "Ans11.\n",
        "Step 1: Create an AWS Glue Crawler\n",
        "\n",
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "# Load data from Glue catalog table (created by crawler)\n",
        "datasource0 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"my_glue_db\",\n",
        "    table_name=\"my_csv_table\",\n",
        "    transformation_ctx=\"datasource0\"\n",
        ")\n",
        "\n",
        "# Convert data to parquet format\n",
        "datasink = glueContext.write_dynamic_frame.from_options(\n",
        "    frame=datasource0,\n",
        "    connection_type=\"s3\",\n",
        "    connection_options={\"path\": \"s3://my-bucket/output-data/\"},\n",
        "    format=\"parquet\",\n",
        "    transformation_ctx=\"datasink\"\n",
        ")\n",
        "\n",
        "job.commit()\n",
        "\n",
        "\n",
        "Q12. Explain the difference between Kinesis Data Streams, Kinesis Firehose, and Kinesis Data Analytics. Provide a real-world example of how each would be used\n",
        "\n",
        "Ans12.\n",
        "Here’s a clear comparison of Amazon Kinesis Data Streams, Kinesis Data Firehose, and Kinesis Data Analytics, along with real-world examples for each:\n",
        "\n",
        "| Service                    | Purpose                                          | Processing Type                     | Real-World Use Case                            |\n",
        "| -------------------------- | ------------------------------------------------ | ----------------------------------- | ---------------------------------------------- |\n",
        "| **Kinesis Data Streams**   | Real-time data capture and custom processing     | Developer-managed stream processing | Real-time clickstream or fraud detection       |\n",
        "| **Kinesis Data Firehose**  | Data ingestion and delivery to storage/analytics | Fully managed data delivery         | Log aggregation pipeline to S3 & Redshift      |\n",
        "| **Kinesis Data Analytics** | SQL analytics on streaming data                  | Serverless SQL querying             | Real-time IoT monitoring and anomaly detection |\n",
        "\n",
        "\n",
        "\n",
        "Q13.  What is columnar storage and how does it benefit Redshift performance for analytics workloads\n",
        "\n",
        "Ans13.\n",
        "Columnar storage is a method of organizing data in a database where data is stored column-by-column instead of row-by-row.\n",
        "\n",
        "| Aspect            | Row Storage          | Columnar Storage          |\n",
        "| ----------------- | -------------------- | ------------------------- |\n",
        "| Data Organization | Row-by-row           | Column-by-column          |\n",
        "| Compression       | Less efficient       | Highly efficient          |\n",
        "| Query Performance | Reads all columns    | Reads only needed columns |\n",
        "| Ideal Use Case    | OLTP (transactional) | OLAP (analytics)          |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9aF-QofCpfjH"
      }
    }
  ]
}