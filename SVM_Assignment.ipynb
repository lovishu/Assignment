{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF954OuOpSC2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.  What is a Support Vector Machine (SVM)\n",
        "\n",
        "Ans1. A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks. It is especially effective in high-dimensional spaces and with clear margin separation.\n",
        "\n",
        "SVM aims to find the best decision boundary (hyperplane) that maximally separates classes in the feature space.\n",
        "\n",
        "For binary classification, SVM finds the hyperplane that:\n",
        "\n",
        "Uses only the support vectors (data points closest to the boundary) to define the decision boundary.\n",
        "\n",
        "Types of SVM:\n",
        "Linear SVM ‚Äì Works when data is linearly separable.\n",
        "\n",
        "Non-Linear SVM ‚Äì Uses kernels to separate complex data.\n",
        "\n",
        "\n",
        "\n",
        "Q2.  What is the difference between Hard Margin and Soft Margin SVM\n",
        "\n",
        "Ans2. Support Vector Machines (SVMs) can be categorized into Hard Margin and Soft Margin approaches based on how strictly they enforce the separation between classes.\n",
        "\n",
        "| Aspect                 | **Hard Margin SVM**                             | **Soft Margin SVM**                                  |\n",
        "| ---------------------- | ----------------------------------------------- | ---------------------------------------------------- |\n",
        "| **Tolerance to Error** | Does **not** allow any misclassifications       | Allows **some** misclassifications (violations)      |\n",
        "| **Assumes**            | Data is **perfectly linearly separable**        | Data is **not perfectly separable**                  |\n",
        "| **Margin**             | Maximizes the margin **without** any violations | Maximizes the margin **with penalty** for violations |\n",
        "| **Flexibility**        | Less flexible, sensitive to noise and outliers  | More flexible, **robust to outliers**                |\n",
        "| **Use Case**           | Rare in real-world scenarios                    | Common in practice                                   |\n",
        "| **Controlled by**      | No slack variables or penalty term              | Introduces **slack variables** and **C parameter**   |\n",
        "\n",
        "\n",
        "Q3.  What is the mathematical intuition behind SVM\n",
        "\n",
        "Ans3. Mathematical Intuition Behind SVM (Support Vector Machine)\n",
        "\n",
        "\n",
        "1. Hyperplane Equation\n",
        "\n",
        "\n",
        "\n",
        "Q4.  What is the role of Lagrange Multipliers in SVM\n",
        "Role of Lagrange Multipliers in SVM\n",
        "Lagrange multipliers play a crucial role in solving the optimization problem at the heart of Support Vector Machines (SVMs). They help convert the constrained optimization problem into a form that is easier to solve ‚Äî specifically, the dual problem.\n",
        "\n",
        "üîß SVM Optimization Setup (Hard Margin)\n",
        "\n",
        "\n",
        "| Concept                     | Role of Lagrange Multipliers                        |\n",
        "| --------------------------- | --------------------------------------------------- |\n",
        "| Enforcing constraints       | Transform inequality constraints into dual form     |\n",
        "| Solving optimization        | Convert primal problem into easier dual form        |\n",
        "| Identifying support vectors | Only points with $\\alpha_i > 0$ are support vectors |\n",
        "| Working with kernels        | Dual form allows use of kernel functions            |\n",
        "\n",
        "\n",
        "Q5.  What are Support Vectors in SVM\n",
        "\n",
        "Ans5. Support Vectors are the critical data points in a dataset that lie closest to the decision boundary (hyperplane) in a Support Vector Machine (SVM) model.\n",
        "\n",
        "Define the optimal hyperplane.\n",
        "\n",
        "Determine the margin.\n",
        "\n",
        "Are the only points used to build the decision function.\n",
        "\n",
        "\n",
        "\n",
        "Q6.  What is a Support Vector Classifier (SVC)\n",
        "\n",
        "Ans6.Support Vectors are the critical data points in a dataset that lie closest to the decision boundary (hyperplane) in a Support Vector Machine (SVM) model.\n",
        "\n",
        "Define the optimal hyperplane.\n",
        "\n",
        "Determine the margin.\n",
        "\n",
        "Are the only points used to build the decision function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q7.  What is a Support Vector Regressor (SVR)\n",
        "\n",
        "Ans7. Support Vector Regressor (SVR) is the regression version of Support Vector Machine (SVM). Instead of classifying data, SVR predicts a continuous target value while trying to keep the predictions within a specified error margin (Œµ-tube) from the actual values.\n",
        "\n",
        "üìå Key Idea\n",
        "\n",
        "SVR tries to fit the best line (or curve) such that the predicted values deviate from the actual values by no more than Œµ (epsilon). It also aims to keep the model as flat (simple) as possible.\n",
        "\n",
        "\n",
        "üîß SVR Optimization (Soft Margin)\n",
        "\n",
        "| SVR Component   | Purpose                                 |\n",
        "| --------------- | --------------------------------------- |\n",
        "| $\\varepsilon$   | Tolerance margin for error              |\n",
        "| $C$             | Penalty for errors beyond $\\varepsilon$ |\n",
        "| Support Vectors | Points outside the Œµ-tube               |\n",
        "| Kernels         | Handle non-linear relationships         |\n",
        "\n",
        "\n",
        "Q8.  What is the Kernel Trick in SVM\n",
        "\n",
        "Ans8. The Kernel Trick is a mathematical technique that allows Support Vector Machines (SVM) to learn non-linear decision boundaries without explicitly transforming the input features into higher dimensions.\n",
        "\n",
        "üîç The Problem\n",
        "SVMs are inherently linear classifiers. But what if your data is not linearly separable?\n",
        "\n",
        "Solution: Map the original features to a higher-dimensional space where the data becomes linearly separable.\n",
        "\n",
        "But computing the transformation explicitly (e.g., converting 2D features to 1000D) is computationally expensive.\n",
        "\n",
        "\n",
        "Q9.  Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
        "\n",
        "Ans9. Here is a comparison of the Linear, Polynomial, and RBF (Radial Basis Function) kernels, the most commonly used kernels in Support Vector Machines (SVM).\n",
        "\n",
        "| Feature             | **Linear Kernel**       | **Polynomial Kernel**      | **RBF Kernel**                 |\n",
        "| ------------------- | ----------------------- | -------------------------- | ------------------------------ |\n",
        "| Formula             | $x^\\top x'$             | $(x^\\top x' + c)^d$        | $\\exp(-\\gamma \\|x - x'\\|^2)$   |\n",
        "| Non-linearity       | ‚ùå No                    | ‚úÖ Yes (degree-controlled)  | ‚úÖ‚úÖ Highly non-linear           |\n",
        "| Parameters          | None                    | $c, d$                     | $\\gamma$                       |\n",
        "| Speed               | ‚úÖ Fast                  | ‚ö†Ô∏è Slower                  | ‚ö†Ô∏è Slowest (most computation)  |\n",
        "| Risk of Overfitting | ‚ùå Low                   | ‚ö†Ô∏è Medium (if high degree) | ‚ö†Ô∏è High (if poorly tuned)      |\n",
        "| Typical Use Cases   | Text data, linear tasks | Curved relationships       | Most classification/regression |\n",
        "\n",
        "\n",
        "Q10.  What is the effect of the C parameter in SVM\n",
        "\n",
        "Ans10.\n",
        "In Support Vector Machines (SVM), the C parameter is a regularization parameter that controls the trade-off between:\n",
        "\n",
        "\n",
        "| Value of `C` | Margin Width | Training Error Tolerance | Risk of Overfitting | Generalization         |\n",
        "| ------------ | ------------ | ------------------------ | ------------------- | ---------------------- |\n",
        "| **High `C`** | Narrow       | Low                      | High                | Poor (can overfit)     |\n",
        "| **Low `C`**  | Wide         | High                     | Low                 | Better (if tuned well) |\n",
        "\n",
        "üß† Intuition\n",
        "Think of C as the cost of violating the margin:\n",
        "\n",
        "Higher C = ‚ÄúDon‚Äôt violate it!‚Äù ‚Üí Fewer slack variables ‚Üí Strict.\n",
        "\n",
        "Lower C = ‚ÄúMistakes are okay‚Äù ‚Üí More flexibility in fitting.\n",
        "\n",
        "\n",
        "\n",
        "Q11.  What is the role of the Gamma parameter in RBF Kernel SVM\n",
        "\n",
        "Ans11. In an SVM using the RBF (Radial Basis Function) kernel, the gamma parameter defines how far the influence of a single training example reaches ‚Äî i.e., it controls the curvature of the decision boundary.\n",
        "\n",
        "| Parameter  | Effect                                                      |\n",
        "| ---------- | ----------------------------------------------------------- |\n",
        "| **High Œ≥** | High model complexity, risk of overfitting                  |\n",
        "| **Low Œ≥**  | Low model complexity, risk of underfitting                  |\n",
        "| **Œ≥ + C**  | Together control the **bias-variance trade-off** in RBF-SVM |\n",
        "\n",
        "\n",
        "\n",
        "Q12.  What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"\n",
        "\n",
        "Ans12. Na√Øve Bayes is a probabilistic classification algorithm based on applying Bayes' theorem with a strong (na√Øve) assumption of feature independence.\n",
        "\n",
        "| Aspect          | Description                                             |\n",
        "| --------------- | ------------------------------------------------------- |\n",
        "| **Type**        | Probabilistic classifier                                |\n",
        "| **Based on**    | Bayes' theorem                                          |\n",
        "| **Assumption**  | Features are conditionally independent given the class  |\n",
        "| **Why \"Na√Øve\"** | Simplifying assumption of independence                  |\n",
        "| **Strength**    | Simple, fast, works well with high-dimensional data     |\n",
        "| **Common use**  | Text classification, spam filtering, sentiment analysis |\n",
        "\n",
        "\n",
        "\n",
        "Q13.  What is Bayes‚Äô Theorem\n",
        "\n",
        "Ans13. Bayes‚Äô Theorem is a fundamental rule in probability theory that describes how to update the probability of a hypothesis based on new evidence.\n",
        "\n",
        "Q14.  Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes\n",
        "\n",
        "Ans14. 1. Gaussian Na√Øve Bayes\n",
        "Assumption: Features are continuous and follow a Gaussian (normal) distribution.\n",
        "\n",
        "Use case: When features are real-valued (e.g., height, weight, temperature).\n",
        "\n",
        "Example: Predicting species from measurements like sepal length/width (Iris dataset).\n",
        "\n",
        "2. Multinomial Na√Øve Bayes\n",
        "Assumption: Features represent counts or frequencies (discrete non-negative values).\n",
        "\n",
        "Use case: Commonly used in text classification where features are word counts or term frequencies.\n",
        "\n",
        "3. Bernoulli Na√Øve Bayes\n",
        "Assumption: Features are binary (0/1), representing presence or absence.\n",
        "\n",
        "Use case: Text classification when you care about whether a word appears or not, regardless of frequency.\n",
        "\n",
        "\n",
        "\n",
        "Q15. When should you use Gaussian Na√Øve Bayes over other variants\n",
        "\n",
        "Ans15.\n",
        "Use Gaussian Na√Øve Bayes when your features are continuous numerical variables that are roughly normally (Gaussian) distributed within each class.\n",
        "\n",
        "When to prefer Gaussian Na√Øve Bayes:\n",
        "\n",
        "Data type: Features are continuous real numbers (e.g., height, weight, temperature, sensor readings).\n",
        "\n",
        "Distribution: Each feature‚Äôs values in each class approximately follow a bell-shaped curve (normal distribution).\n",
        "\n",
        "| Scenario                                | Recommended Na√Øve Bayes Variant |\n",
        "| --------------------------------------- | ------------------------------- |\n",
        "| Continuous, approximately Gaussian data | **Gaussian Na√Øve Bayes**        |\n",
        "| Discrete counts (e.g., word counts)     | Multinomial Na√Øve Bayes         |\n",
        "| Binary features (presence/absence)      | Bernoulli Na√Øve Bayes           |\n",
        "\n",
        "\n",
        "Q16.  What are the key assumptions made by Na√Øve Bayes\n",
        "\n",
        "Ans16. 1. Conditional Independence\n",
        "Assumption: All features are conditionally independent given the class label.\n",
        "\n",
        "Meaning: The presence or value of one feature does not affect or provide information about another feature once the class is known.\n",
        "\n",
        "2. Feature Distribution Assumption\n",
        "Na√Øve Bayes assumes a specific probability distribution for features depending on the variant:\n",
        "\n",
        "| Assumption                   | Description                                        |\n",
        "| ---------------------------- | -------------------------------------------------- |\n",
        "| **Conditional Independence** | Features independent given the class               |\n",
        "| **Feature Distribution**     | Features follow assumed distribution per variant   |\n",
        "| **Accurate Class Priors**    | Class probabilities represent true prior knowledge |\n",
        "\n",
        "\n",
        "Q17.  What are the advantages and disadvantages of Na√Øve Bayes\n",
        "\n",
        "Ans17. Advantages\n",
        "Simple and Fast\n",
        "\n",
        "Works Well with High-Dimensional Data\n",
        "\n",
        "Requires Less Training Data\n",
        "\n",
        "\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "\n",
        "Strong Independence Assumption\n",
        "\n",
        "Poor Performance with Correlated Features\n",
        "\n",
        "Zero Frequency Problem\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q18.  Why is Na√Øve Bayes a good choice for text classification\n",
        "\n",
        "Ans18.\n",
        "\n",
        "1. Handles High-Dimensional Data Efficiently\n",
        "Text data typically involves thousands of features (words or tokens).\n",
        "\n",
        "Na√Øve Bayes scales well to this high-dimensional space without heavy computational cost.\n",
        "\n",
        "\n",
        "\n",
        "2. Works Well with Sparse Data\n",
        "Most documents contain only a small subset of all possible words (sparse feature vectors).\n",
        "\n",
        "Na√Øve Bayes handles sparse inputs naturally, especially the Multinomial and Bernoulli variants.\n",
        "\n",
        "\n",
        "Q19. Compare SVM and Na√Øve Bayes for classification tasks\n",
        "\n",
        "Ans19.\n",
        "\n",
        "| Aspect                            | SVM                                                                   | Na√Øve Bayes                                                 |\n",
        "| --------------------------------- | --------------------------------------------------------------------- | ----------------------------------------------------------- |\n",
        "| **Type of Model**                 | Discriminative, margin-based classifier                               | Generative probabilistic classifier                         |\n",
        "| **Assumptions**                   | Makes no strong assumptions about feature independence                | Assumes conditional independence of features                |\n",
        "| **Handling of Data**              | Effective with both linear and nonlinear data (via kernels)           | Works well with high-dimensional, sparse data (e.g., text)  |\n",
        "| **Training Complexity**           | Computationally intensive, especially for large datasets              | Very fast and simple to train                               |\n",
        "| **Interpretability**              | Harder to interpret decision boundary                                 | Outputs probabilities, easier to interpret                  |\n",
        "| **Handling Non-linearities**      | Uses kernel trick to model complex boundaries                         | Limited to model complexity via feature transformations     |\n",
        "| **Robustness to Noise/Outliers**  | Can be sensitive to noise, but soft margin SVM handles this           | Less sensitive to noise due to probabilistic nature         |\n",
        "| **Performance on Small Datasets** | Performs well, especially with well-chosen kernels                    | May struggle if feature independence assumption is violated |\n",
        "| **Output**                        | Hard classification boundary (decision function)                      | Probabilistic output (class probabilities)                  |\n",
        "| **Use Cases**                     | Image recognition, bioinformatics, text classification (with kernels) | Text classification, spam filtering, sentiment analysis     |\n",
        "\n",
        "\n",
        "Q20.  How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "\n",
        "Ans20.\n",
        "Laplace smoothing (also called add-one smoothing) is a technique used to handle the zero-frequency problem in Na√Øve Bayes.\n",
        "\n",
        "The Zero-Frequency Problem\n",
        "When a feature (e.g., a word) never appears in the training data for a given class, its probability estimate becomes zero.\n",
        "\n",
        "Since Na√Øve Bayes multiplies probabilities of all features, this zero probability causes the entire product to become zero, making it impossible to predict that class.\n",
        "\n",
        "\n",
        "Pracical\n",
        "\n",
        "Q21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "\n",
        "\n",
        "Ans21. from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize SVM classifier (default RBF kernel)\n",
        "svm_clf = SVC()\n",
        "\n",
        "# Train the model\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Classifier Accuracy on Iris test set: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "Q22.  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies\n",
        "\n",
        "\n",
        "Ans22.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize SVM classifiers with Linear and RBF kernels\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "\n",
        "# Train both models\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy with Linear kernel: {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy with RBF kernel: {accuracy_rbf:.2f}\")\n",
        "\n",
        "\n",
        "Q23.  Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "\n",
        "Ans3. from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize SVM classifiers with Linear and RBF kernels\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "\n",
        "# Train both models\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy with Linear kernel: {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy with RBF kernel: {accuracy_rbf:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q24.  Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary\n",
        "\n",
        "Ans24.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load Iris dataset and select two features for easy visualization\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Use first two features\n",
        "y = iris.target\n",
        "\n",
        "# Train SVM classifier with polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
        "svm_poly.fit(X, y)\n",
        "\n",
        "# Create a mesh grid for plotting decision boundaries\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
        "                     np.linspace(y_min, y_max, 500))\n",
        "\n",
        "# Predict class labels for each point in the mesh\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary and training points\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Set1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Set1, edgecolors='k')\n",
        "plt.xlabel(iris.feature_names[0])\n",
        "plt.ylabel(iris.feature_names[1])\n",
        "plt.title(\"SVM with Polynomial Kernel (degree=3)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q25.  Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and\n",
        "\n",
        "Ans25.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split dataset into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Gaussian Naive Bayes accuracy on Breast Cancer dataset: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "Q26.  Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20 Newsgroups dataset.\n",
        "\n",
        "Ans26. from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load 20 Newsgroups dataset (subset for faster training)\n",
        "newsgroups = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split dataset into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize Multinomial Naive Bayes classifier\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "mnb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = mnb.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Multinomial Naive Bayes accuracy on 20 Newsgroups dataset: {accuracy:.2f}\")\n",
        "\n",
        "# Optional: Print classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=newsgroups.target_names))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7gBSR8pkqTO1"
      }
    }
  ]
}