{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1IlUI_HXiGI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.  What does R-squared represent in a regression model\n",
        "\n",
        "Ans1.R-squared (R¬≤), also known as the coefficient of determination, represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model.\n",
        "\n",
        "Interpretation:\n",
        "R¬≤ = 1 ‚Üí The model perfectly explains all the variability in the response data.\n",
        "\n",
        "R¬≤ = 0 ‚Üí The model explains none of the variability in the response data.\n",
        "\n",
        "Notes:\n",
        "\n",
        "A higher R¬≤ generally indicates a better fit, but it does not imply causation.\n",
        "\n",
        "For multiple regression, adding more predictors will always increase R¬≤, even if they are not meaningful. That's why Adjusted R¬≤ is used to account for the number of predictors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q2. What are the assumptions of linear regression\n",
        "\n",
        "Ans2. The assumptions of linear regression ensure that the model provides valid, reliable, and unbiased estimates. Here are the key assumptions:\n",
        "\n",
        "1. Linearity\n",
        "The relationship between the independent variables and the dependent variable is linear.\n",
        "\n",
        "2. Independence of Errors\n",
        "The residuals (errors) are independent of each other.\n",
        "\n",
        "3. Homoscedasticity (Constant Variance of Errors)\n",
        "The variance of residuals is constant across all levels of the independent variables.\n",
        "\n",
        "If not met, it leads to heteroscedasticity, which affects the efficiency of estimates.\n",
        "\n",
        "4. Normality of Errors\n",
        "The residuals are normally distributed (especially important for valid hypothesis testing and confidence intervals).\n",
        "\n",
        "This assumption is less critical for prediction but important for inference.\n",
        "\n",
        "5. No Multicollinearity\n",
        "The independent variables are not highly correlated with each other.\n",
        "\n",
        "High multicollinearity inflates the variance of coefficient estimates and makes them unstable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q3.  What is the difference between R-squared and Adjusted R-squared\n",
        "\n",
        "Ans3.\n",
        "\n",
        "| **Aspect**                      | **R-squared (R¬≤)**                                                                            | **Adjusted R-squared**                                                                                                                                         |\n",
        "| ------------------------------- | --------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Definition**                  | Measures the proportion of variance explained by the model.                                   | Adjusts R¬≤ based on the number of predictors.                                                                                                                  |\n",
        "| **Formula**                     | $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$                                                         | $\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)$ <br>where:<br> *$n$* = number of observations,<br> *$k$* = number of predictors |\n",
        "| **Effect of Adding Predictors** | Always increases or stays the same (never decreases)                                          | Can **increase or decrease** depending on whether the new predictor improves the model significantly                                                           |\n",
        "| **Overfitting Check**           | Can be misleading ‚Äî may suggest a better model even if the added variables are not meaningful | Penalizes unnecessary predictors ‚Äî gives a **more accurate model fit**                                                                                         |\n",
        "| **Usefulness**                  | Good for simple models or when comparing models with the same number of predictors            | Better for **multiple regression**, especially when comparing models with **different numbers of predictors**                                                  |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q4.  Why do we use Mean Squared Error (MSE)\n",
        "\n",
        "Ans4. We use Mean Squared Error (MSE) in regression analysis to quantify the average squared difference between predicted and actual values.\n",
        "\n",
        "‚úÖ Why We Use MSE:\n",
        "\n",
        "\n",
        "Measures Accuracy of Predictions:\n",
        "\n",
        "Penalizes Larger Errors More:\n",
        "\n",
        "Differentiable:\n",
        "\n",
        "Widely Used Benchmark:\n",
        "\n",
        "\n",
        "Q5.  What does an Adjusted R-squared value of 0.85 indicate\n",
        "\n",
        "Ans5. An Adjusted R-squared value of 0.85 indicates that:\n",
        "\n",
        "85% of the variability in the dependent variable is explained by the independent variables after adjusting for the number of predictors in the model.\n",
        "\n",
        "üîç What It Means Practically:\n",
        "The model explains a large proportion of the variance in the outcome variable.\n",
        "\n",
        "The adjustment accounts for the number of predictors, so this high value suggests that:\n",
        "\n",
        "Most predictors are contributing meaningfully.\n",
        "\n",
        "The model is not overfitting due to unnecessary variables.\n",
        "\n",
        "\n",
        "üìå Summary:\n",
        "An Adjusted R¬≤ = 0.85 reflects a well-fitting regression model that captures the majority of the variation in the dependent variable, without being over-complex.\n",
        "\n",
        "\n",
        "Q6. How do we check for normality of residuals in linear regression\n",
        "\n",
        "Ans6.To check for normality of residuals in a linear regression model, you want to assess whether the errors (residuals) follow a normal distribution ‚Äî a key assumption for valid statistical inference (e.g., p-values, confidence intervals).\n",
        "\n",
        "‚úÖ Methods to Check Normality of Residuals:\n",
        "\n",
        "1. Histogram\n",
        "\n",
        "2. Q‚ÄìQ Plot (Quantile‚ÄìQuantile Plot)\n",
        "\n",
        "3. Shapiro‚ÄìWilk Test\n",
        "\n",
        "4. Kolmogorov‚ÄìSmirnov Test / Anderson‚ÄìDarling Test\n",
        "\n",
        "5. Skewness & Kurtosis\n",
        "\n",
        "\n",
        "Q7.  What is multicollinearity, and how does it impact regression\n",
        "\n",
        "\n",
        "Ans7.\n",
        "Multicollinearity occurs when two or more independent variables in a regression model are highly correlated ‚Äî meaning they contain similar information about the variance in the dependent variable.\n",
        "\n",
        "\n",
        "‚ö†Ô∏è Why It‚Äôs a Problem\n",
        "\n",
        "Unstable Coefficient Estimates\n",
        "\n",
        "Inflated Standard Errors\n",
        "\n",
        "üîç How to Detect Multicollinearity\n",
        "\n",
        "1. Variance Inflation Factor (VIF)\n",
        "\n",
        "\n",
        "üõ†Ô∏è How to Handle Multicollinearity\n",
        "\n",
        "Remove Redundant Predictors\n",
        "\n",
        "Combine Variables\n",
        "\n",
        "Regularization Techniques\n",
        "\n",
        "\n",
        "\n",
        "Q8.  What is Mean Absolute Error (MAE)\n",
        "\n",
        "Ans8.\n",
        "üìå Mean Absolute Error (MAE)\n",
        "Mean Absolute Error (MAE) is a regression metric that measures the average absolute difference between the actual values and the predicted values.\n",
        "\n",
        "‚úÖ Use Cases:\n",
        "Prefer MAE when:\n",
        "\n",
        "You want a simple, interpretable metric in the same units as the target.\n",
        "\n",
        "You want to treat all errors equally (no heavy penalty for large errors).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q9.  What are the benefits of using an ML pipeline\n",
        "\n",
        "Ans9.\n",
        "\n",
        "‚úÖ Benefits of Using a Machine Learning (ML) Pipeline\n",
        "An ML pipeline automates and streamlines the steps in a machine learning workflow ‚Äî from data preprocessing to model deployment. Using pipelines offers numerous advantages:\n",
        "\n",
        "1. Modularity and Reusability\n",
        "Breaks the ML process into independent, reusable steps (e.g., preprocessing, training, evaluation).\n",
        "\n",
        "You can swap out or tune individual components without rebuilding the whole workflow.\n",
        "\n",
        "2. Consistency and Reproducibility\n",
        "Ensures that the same sequence of steps is applied to both training and testing data.\n",
        "\n",
        "3. Automation\n",
        "Automates the end-to-end ML workflow.\n",
        "\n",
        "Enables batch training, cross-validation, hyperparameter tuning, and deployment with minimal manual effort.\n",
        "\n",
        "4. Improved Collaboration\n",
        "\n",
        "\n",
        "Pipelines provide a standardized structure that teams can understand and work on together.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q10.  Why is RMSE considered more interpretable than MSE\n",
        "\n",
        "Ans10.\n",
        "\n",
        "üß† Key Reason: Units of Measurement\n",
        "MSE expresses error in squared units of the target variable (e.g., if the target is in kg, MSE is in kg¬≤).\n",
        "\n",
        "RMSE takes the square root of MSE, so the result is in the same units as the target variable (e.g., kg).\n",
        "\n",
        "\n",
        "‚û°Ô∏è This makes RMSE more directly interpretable in the context of the actual problem.\n",
        "\n",
        "\n",
        "Q11.  What is pickling in Python, and how is it useful in ML\n",
        "\n",
        "Ans11. Pickling is the process of serializing a Python object ‚Äî converting it into a byte stream so it can be saved to a file or transferred over a network.\n",
        "\n",
        "The opposite process, unpickling, converts the byte stream back into the original Python object.\n",
        "\n",
        "Why is Pickling Useful in Machine Learning?\n",
        "Model Persistence\n",
        "\n",
        "Reproducibility\n",
        "\n",
        "Efficiency\n",
        "\n",
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# Save model\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Load model\n",
        "with open('model.pkl', 'rb') as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "\n",
        "# Use loaded model for prediction\n",
        "predictions = loaded_model.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "Q12.  What does a high R-squared value mean\n",
        "\n",
        "\n",
        "Ans12. A high R-squared value means that a large proportion of the variance in the dependent variable is explained by the independent variables in the regression model.\n",
        "\n",
        "In summary:\n",
        "High R-squared = good fit / high explanatory power.\n",
        "\n",
        "Always consider Adjusted R-squared and other diagnostics to ensure the model is reliable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q13.  What happens if linear regression assumptions are violated\n",
        "\n",
        "Ans13.If linear regression assumptions are violated, the reliability and validity of the model‚Äôs results can be compromised. Here‚Äôs what can happen for each assumption violation:\n",
        "\n",
        "1. Linearity Violation\n",
        "2. Independence of Errors Violation\n",
        "3. Homoscedasticity Violation (Heteroscedasticity)\n",
        "4. Normality of Errors Violation\n",
        "5. Multicollinearity\n",
        "6. Measurement Error in Predictors\n",
        "\n",
        "\n",
        "\n",
        "Q14.  How can we address multicollinearity in regression\n",
        "\n",
        "Ans14. To address multicollinearity in regression, you can apply several strategies that reduce correlation among predictors or mitigate its effects:\n",
        "\n",
        "1. Remove Highly Correlated Predictors\n",
        "2. Combine Variables\n",
        "3. Use Principal Component Analysis (PCA)\n",
        "4. Regularization Techniques\n",
        "5. Increase Sample Size\n",
        "6. Centering Variables\n",
        "\n",
        "\n",
        "\n",
        "Q15.  How can feature selection improve model performance in regression analysis\n",
        "\n",
        "Ans15. 1. Reduces Overfitting\n",
        "By selecting only relevant features, the model becomes simpler and less likely to fit noise.\n",
        "\n",
        "This improves generalization to unseen data.\n",
        "\n",
        "2. Improves Model Interpretability\n",
        "3. Decreases Training Time and Complexity\n",
        "\n",
        "\n",
        "| Benefit               | Explanation                           |\n",
        "| --------------------- | ------------------------------------- |\n",
        "| Overfitting Reduction | Less noise, better generalization     |\n",
        "| Interpretability      | Simpler, clearer model                |\n",
        "| Efficiency            | Faster training and prediction        |\n",
        "| Multicollinearity     | More stable and reliable coefficients |\n",
        "| Accuracy              | Better predictive performance         |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q16.  How is Adjusted R-squared calculated\n",
        "\n",
        "Ans16.\n",
        "Adjusted R-squared modifies the regular R-squared to account for the number of predictors in the model, penalizing the addition of irrelevant variables.\n",
        "\n",
        "Explanation:\n",
        "When you add more predictors, regular R¬≤ never decreases (it can only increase or stay the same).\n",
        "\n",
        "Adjusted R¬≤ increases only if the new predictor improves the model more than would be expected by chance.\n",
        "\n",
        "It can decrease if unnecessary variables are added, helping to prevent overfitting.\n",
        "\n",
        "\n",
        "\n",
        "Q17.  Why is MSE sensitive to outliers\n",
        "\n",
        "Ans17. Key Reason: Squaring the Errors\n",
        "Squaring errors means:\n",
        "\n",
        "Larger errors become disproportionately more significant because they are squared.\n",
        "\n",
        "| Metric | Sensitivity to Outliers | Reason               |\n",
        "| ------ | ----------------------- | -------------------- |\n",
        "| MSE    | High                    | Errors are squared   |\n",
        "| MAE    | Lower                   | Uses absolute errors |\n",
        "\n",
        "\n",
        "Q18.  What is the role of homoscedasticity in linear regression\n",
        "\n",
        "\n",
        "Ans18.Homoscedasticity means the variance of the residuals (errors) is constant across all levels of the independent variables.\n",
        "\n",
        "\n",
        "The spread of errors should be roughly the same whether the predicted value is low or high.\n",
        "\n",
        "Why is Homoscedasticity Important?\n",
        "Valid Standard Errors\n",
        "\n",
        "Constant error variance ensures that the standard errors of the coefficients are accurate.\n",
        "\n",
        "Accurate standard errors are essential for reliable hypothesis testing (t-tests, confidence intervals).\n",
        "\n",
        "\n",
        "\n",
        "Efficient Estimators\n",
        "\n",
        "\n",
        "Under homoscedasticity, Ordinary Least Squares (OLS) estimators are the Best Linear Unbiased Estimators (BLUE).\n",
        "\n",
        "If variance is not constant, OLS is still unbiased but not efficient (larger variance than necessary).\n",
        "\n",
        "\n",
        "\n",
        "Q19.  What is Root Mean Squared Error (RMSE)\n",
        "\n",
        "Ans19. RMSE is a popular metric to measure the average magnitude of the errors between predicted and actual values in regression problems.\n",
        "\n",
        "Use Cases:\n",
        "Commonly used to evaluate regression models.\n",
        "\n",
        "Sensitive to large errors because of the squaring term.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q20. Why is pickling considered risky\n",
        "\n",
        "Ans20. Pickling in Python involves serializing objects into byte streams, which can later be deserialized (unpickled). However, this process has inherent risks:\n",
        "\n",
        "1. Security Risk: Arbitrary Code Execution\n",
        "2. Lack of Compatibility\n",
        "3. Data Corruption\n",
        "Pickled files are binary and not human-readable.\n",
        "\n",
        "Corruption in the file may cause unpickling to fail or produce incorrect objects.\n",
        "\n",
        "Best Practices:\n",
        "Never unpickle data from untrusted or unauthenticated sources.\n",
        "\n",
        "Use safer alternatives for model serialization in ML like joblib (for large numpy arrays) or standardized formats like ONNX.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q21. What alternatives exist to pickling for saving ML models\n",
        "\n",
        "Ans21.\n",
        "\n",
        "Here are some popular and safer alternatives to pickle for saving machine learning models:\n",
        "\n",
        "1. Joblib\n",
        "2. ONNX (Open Neural Network Exchange)\n",
        "3. HDF5 / h5py\n",
        "\n",
        "| Method               | Use Case                         | Pros                              | Cons                             |\n",
        "| -------------------- | -------------------------------- | --------------------------------- | -------------------------------- |\n",
        "| **Joblib**           | scikit-learn models              | Fast, efficient with numpy arrays | Python-specific                  |\n",
        "| **ONNX**             | Cross-framework interoperability | Standardized, portable            | Requires conversion steps        |\n",
        "| **HDF5 (Keras)**     | Deep learning models             | Stores weights & architecture     | More complex setup               |\n",
        "| **JSON + Weights**   | Model architecture + parameters  | Human-readable architecture       | Requires managing multiple files |\n",
        "| **Framework-native** | TensorFlow, PyTorch              | Fully supported by frameworks     | Framework dependent              |\n",
        "\n",
        "\n",
        "\n",
        "Q22.  What is heteroscedasticity, and why is it a problem\n",
        "\n",
        "Ans22. Heteroscedasticity occurs when the variance of the residuals (errors) in a regression model is not constant across all levels of the independent variable(s).\n",
        "\n",
        "In other words, the spread of errors changes (increases or decreases) with the value of predictors or fitted values.\n",
        "\n",
        "This violates the assumption of homoscedasticity (constant variance) in linear regression.\n",
        "\n",
        "Why is Heteroscedasticity a Problem?\n",
        "How to Address It?\n",
        "Use heteroscedasticity-robust standard errors (e.g., White‚Äôs correction).\n",
        "\n",
        "Transform dependent variable (e.g., log transformation).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q23.  How can interaction terms enhance a regression model's predictive power?\n",
        "\n",
        "Ans23. Interaction terms represent the combined effect of two or more predictors on the target variable.\n",
        "\n",
        "Instead of assuming each predictor‚Äôs effect is independent, interaction terms model how the effect of one predictor depends on the level of another.\n",
        "\n",
        "Important Considerations:\n",
        "Adding many interaction terms can increase model complexity and risk overfitting.\n",
        "\n",
        "Always check if interaction terms significantly improve the model using statistical tests or cross-validation.\n",
        "\n",
        "\n",
        "\n",
        "Practical\n",
        "\n",
        "Q1.  1. Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model\n",
        "using Seaborn's \"diamonds\" dataset.\n",
        "\n",
        "Ans1.\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Select features and target\n",
        "# Use numeric features only for simplicity (carat, depth, table, x, y, z)\n",
        "features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
        "X = diamonds[features]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split data into train and test to avoid data leakage (optional but good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Fit multiple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Plot distribution of residuals\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(residuals, kde=True, bins=50, color='skyblue')\n",
        "plt.title('Distribution of Residuals (Errors) for Multiple Linear Regression Model')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "Q2.  2. Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root\n",
        "Mean Squared Error (RMSE) for a linear regression model.\n",
        "\n",
        "Ans2.\n",
        "\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Load diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Select numeric features for regression\n",
        "features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
        "X = diamonds[features]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Initialize and train linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate errors\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the error metrics\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "Q3.  3. Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check  linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity.\n",
        "\n",
        "\n",
        "Ans3.\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Select numeric features and target\n",
        "features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
        "X = diamonds[features]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# 1. Scatter plot for linearity (Predicted vs Actual)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(y_pred, y_test, alpha=0.5)\n",
        "plt.xlabel('Predicted Price')\n",
        "plt.ylabel('Actual Price')\n",
        "plt.title('Scatter Plot: Predicted vs Actual (Linearity Check)')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Diagonal line\n",
        "plt.show()\n",
        "\n",
        "# 2. Residuals plot for homoscedasticity (Residuals vs Predicted)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Price')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Predicted (Homoscedasticity Check)')\n",
        "plt.show()\n",
        "\n",
        "# 3. Correlation matrix for multicollinearity\n",
        "plt.figure(figsize=(8,6))\n",
        "corr_matrix = X.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Matrix of Features (Multicollinearity Check)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q4.  4. Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the performance of different regression models\n",
        "\n",
        "\n",
        "Ans4.import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Select numeric features and target\n",
        "features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
        "X = diamonds[features]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Define models to evaluate\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=1.0),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Evaluate each model using a pipeline with StandardScaler\n",
        "for name, model in models.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "    \n",
        "    # Train\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    \n",
        "    print(f\"{name}:\\n  R-squared: {r2:.4f}\\n  RMSE: {rmse:.2f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "Q5.  5. Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and R-squared score.\n",
        "\n",
        "\n",
        "Ans5. import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Use single feature 'carat' to predict 'price'\n",
        "X = diamonds[['carat']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Initialize and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print coefficients, intercept, and R-squared\n",
        "print(f\"Coefficient (slope): {model.coef_[0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5UI_eqdkXjoc"
      }
    }
  ]
}