{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn2ba3RXicFC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Simple Linear Regression\n",
        "\n",
        "Ans1. Simple Linear Regression is a statistical method that models the relationship between two variables by fitting a straight line.\n",
        "\n",
        "Key Points:\n",
        "It involves one independent variable (feature)\n",
        "The goal is to find the best-fitting line that predicts\n",
        "Summary\n",
        "Simple Linear Regression fits a straight line through data points to model the relationship between one input and one output variable.\n",
        "\n",
        "\n",
        "Q2.  What are the key assumptions of Simple Linear Regression\n",
        "\n",
        "Ans2.To ensure reliable results, Simple Linear Regression relies on these fundamental assumptions:\n",
        "\n",
        "Linearity\n",
        "\n",
        "The relationship between the independent variable\n",
        "Independence of Errors\n",
        "\n",
        "Independence of Errors\n",
        "\n",
        "The residuals (errors) are independent of each other.\n",
        "\n",
        "No autocorrelation (especially important in time series data).\n",
        "\n",
        "Homoscedasticity (Constant Variance)\n",
        "\n",
        "The variance of the residuals is constant across all values of\n",
        "\n",
        "| Assumption       | Meaning                                    |\n",
        "| ---------------- | ------------------------------------------ |\n",
        "| Linearity        | Relationship between $X$ and $Y$ is linear |\n",
        "| Independence     | Errors are independent                     |\n",
        "| Homoscedasticity | Constant variance of errors                |\n",
        "| Normality        | Errors follow a normal distribution        |\n",
        "\n",
        "\n",
        "\n",
        "Q3.  What does the coefficient m represent in the equation Y=mX+c\n",
        "\n",
        "Ans3. m represents the coefficient (slope) of the independent variable\n",
        "It indicates how much the dependent variable Y changes for a one-unit increase in\n",
        "m shows the rate of change or strength and direction of the relationship between\n",
        "\n",
        "\n",
        "\n",
        "Q4.  What does the intercept c represent in the equation Y=mX+c\n",
        "\n",
        "Ans4. c is the intercept (also called the constant term).\n",
        "\n",
        "Essentially, it‚Äôs the point where the line crosses the Y-axis.\n",
        "c=5, when,\n",
        "\n",
        "X=0, the predicted value of\n",
        "\n",
        "\n",
        "\n",
        "Q5. How do we calculate the slope m in Simple Linear Regression\n",
        "\n",
        "Ans5.\n",
        "Explanation:\n",
        "The numerator is the covariance between\n",
        "X and Y\n",
        "The denominator is the variance of\n",
        "\n",
        "\n",
        "\n",
        "Intuition:\n",
        "X and Y increase together, slope is positive.\n",
        "\n",
        "\n",
        "Q6.  What is the purpose of the least squares method in Simple Linear Regression\n",
        "\n",
        "Ans6.The least squares method is used to find the best-fitting line through the data points by minimizing the total sum of squared differences between the observed values and the predicted values.\n",
        "\n",
        "Summary:\n",
        "The least squares method ensures the line fits the data points in a way that total squared prediction errors are minimized, providing the most accurate linear model.\n",
        "\n",
        "\n",
        "\n",
        "Q7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression\n",
        "\n",
        "Ans7.Interpretation of the Coefficient of Determination (R¬≤) in Simple Linear Regression\n",
        "R¬≤ (R-squared) measures how well the regression line fits the data.\n",
        "\n",
        "| R¬≤ Value | Meaning                                                      |\n",
        "| -------- | ------------------------------------------------------------ |\n",
        "| 0        | The model explains **none** of the variance in $Y$           |\n",
        "| 0.5      | The model explains **50%** of the variance in $Y$            |\n",
        "| 1        | The model explains **all** the variance in $Y$ (perfect fit) |\n",
        "\n",
        "\n",
        "Q8. What is Multiple Linear Regression\n",
        "\n",
        "Ans8. Multiple Linear Regression is an extension of simple linear regression that models the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "Multiple Linear Regression models the linear relationship between a dependent variable and multiple independent variables, allowing for more complex and realistic predictions compared to simple linear regression.\n",
        "\n",
        "\n",
        "\n",
        "Q9. What is the main difference between Simple and Multiple Linear Regression\n",
        "\n",
        "Ans9.\n",
        "| Aspect                          | Simple Linear Regression                           | Multiple Linear Regression                                   |\n",
        "| ------------------------------- | -------------------------------------------------- | ------------------------------------------------------------ |\n",
        "| Number of Independent Variables | Only **one** independent variable ($X$)            | **Two or more** independent variables ($X_1, X_2, ..., X_n$) |\n",
        "| Model Equation                  | $Y = b_0 + b_1 X + \\epsilon$                       | $Y = b_0 + b_1 X_1 + b_2 X_2 + ... + b_n X_n + \\epsilon$     |\n",
        "| Complexity                      | Simpler, models relationship between two variables | More complex, models combined effect of multiple variables   |\n",
        "| Use Case                        | When outcome depends on a single predictor         | When outcome depends on multiple predictors                  |\n",
        "\n",
        "\n",
        "Q10. What are the key assumptions of Multiple Linear Regression\n",
        "\n",
        "\n",
        "Ans10. Multiple Linear Regression relies on these important assumptions to ensure valid and reliable results:\n",
        "\n",
        "Linearity\n",
        "\n",
        "The relationship between the dependent variable and each independent variable is linear.\n",
        "\n",
        "Independence of Errors\n",
        "\n",
        "The residuals (errors) are independent; no autocorrelation.\n",
        "\n",
        "\n",
        "\n",
        "| Assumption           | Description                                        |\n",
        "| -------------------- | -------------------------------------------------- |\n",
        "| Linearity            | Linear relationship between predictors and outcome |\n",
        "| Independence         | Residuals are independent                          |\n",
        "| Homoscedasticity     | Constant variance of residuals                     |\n",
        "| Normality            | Residuals are normally distributed                 |\n",
        "| No Multicollinearity | Predictors are not highly correlated               |\n",
        "| No Outliers          | No influential extreme data points                 |\n",
        "\n",
        "\n",
        "\n",
        "Q11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "\n",
        "Ans11. Heteroscedasticity occurs when the variance of the residuals (errors) is not constant across all levels of the independent variables in a regression model.\n",
        "\n",
        "This violates the assumption of homoscedasticity (constant variance of errors).\n",
        "\n",
        "| Aspect             | Effect of Heteroscedasticity                   |\n",
        "| ------------------ | ---------------------------------------------- |\n",
        "| Variance of Errors | Not constant, changes with predictor values    |\n",
        "| Impact on Model    | Biased standard errors, unreliable tests       |\n",
        "| Consequence        | Wrong inference about predictors' significance |\n",
        "\n",
        "\n",
        "Q12.  How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "\n",
        "Ans12.Multicollinearity occurs when independent variables are highly correlated, causing instability in coefficient estimates and making it hard to assess the effect of each predictor.\n",
        "\n",
        "Ways to Address High Multicollinearity:\n",
        "Remove Highly Correlated Features\n",
        "\n",
        "Identify correlated variables (using correlation matrix or Variance Inflation Factor - VIF).\n",
        "\n",
        "Drop or combine redundant variables.\n",
        "\n",
        "Use Dimensionality Reduction Techniques\n",
        "\n",
        "Principal Component Analysis (PCA): Transforms correlated features into a smaller set of uncorrelated components.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q13. What are some common techniques for transforming categorical variables for use in regression models\n",
        "\n",
        "Ans13.Common Techniques for Transforming Categorical Variables for Regression Models\n",
        "Common Techniques:\n",
        "One-Hot Encoding (Dummy Variables)\n",
        "\n",
        "Creates binary columns for each category (0 or 1).\n",
        "\n",
        "Suitable for nominal categories without order.\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "| Technique          | Use Case                    | Pros                   | Cons                          |\n",
        "| ------------------ | --------------------------- | ---------------------- | ----------------------------- |\n",
        "| One-Hot Encoding   | Nominal categories          | No assumed order       | Can cause high dimensionality |\n",
        "| Label Encoding     | Ordinal categories          | Simple                 | Misleading for nominal data   |\n",
        "| Ordinal Encoding   | Ordinal categories          | Preserves order        | Only for ordered categories   |\n",
        "| Binary Encoding    | High-cardinality categories | Reduces dimensionality | More complex                  |\n",
        "| Frequency Encoding | Categorical importance      | Simple, informative    | Loses category identity       |\n",
        "| Target Encoding    | When target info is useful  | Can improve accuracy   | Risk of data leakage          |\n",
        "\n",
        "\n",
        "\n",
        "Q14.  What is the role of interaction terms in Multiple Linear Regression\n",
        "\n",
        " Ans14.\n",
        "How Interaction Terms Work:\n",
        "If you have two variables , their interaction term is usually represented as:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " | Aspect         | Explanation                                            |\n",
        "| -------------- | ------------------------------------------------------ |\n",
        "| Purpose        | Capture combined effects of predictors                 |\n",
        "| Effect         | Allows the effect of one variable to depend on another |\n",
        "| Improves Model | Increases flexibility and can reveal deeper insights   |\n",
        "\n",
        "\n",
        "\n",
        "Q15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "\n",
        "Ans15.Interpretation of the Intercept in Simple vs. Multiple Linear Regression\n",
        "\n",
        "\n",
        "| Aspect                      | Simple Linear Regression  | Multiple Linear Regression                                                     |\n",
        "| --------------------------- | ------------------------- | ------------------------------------------------------------------------------ |\n",
        "| Number of predictors        | One                       | Multiple                                                                       |\n",
        "| Interpretation of intercept | Value of $Y$ when $X=0$   | Value of $Y$ when **all** $X_i=0$                                              |\n",
        "| Practical meaning           | Often easier to interpret | Sometimes less meaningful if zero is outside the data range for all predictors |\n",
        "\n",
        "\n",
        "\n",
        "Q16. What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "\n",
        "Ans16.The slope (coefficient) in regression quantifies the relationship between an independent variable and the dependent variable.\n",
        "\n",
        "What the Slope Represents:\n",
        "It shows the expected change in the dependent variable Y for a one-unit increase in the independent variable X, holding other variables constant (in multiple regression).\n",
        "\n",
        "\n",
        "| Aspect               | Explanation                               |\n",
        "| -------------------- | ----------------------------------------- |\n",
        "| Role                 | Measures impact of predictor on outcome   |\n",
        "| Significance         | Shows strength and direction of effect    |\n",
        "| Effect on prediction | Changes predicted $Y$ proportional to $X$ |\n",
        "\n",
        "\n",
        "\n",
        "Q17.  How does the intercept in a regression model provide context for the relationship between variables\n",
        "\n",
        "Ans17.The intercept in a regression model represents the baseline value of the dependent variable\n",
        "Y when all independent variables\n",
        "\n",
        "If zero is a meaningful value for predictors, the intercept has a clear real-world interpretation.\n",
        "\n",
        "If zero values for predictors are unrealistic or outside the data range, the intercept may be less meaningful but still necessary mathematically.\n",
        "\n",
        "Example:\n",
        "In predicting salary based on years of experience, the intercept might represent the starting salary when experience = 0 years.\n",
        "\n",
        "In predicting house prices based on size and location scores, the intercept is the expected price when size and location scores are zero (may not be practical but necessary for the equation).\n",
        "\n",
        "Summary:\n",
        "Aspect\tRole of Intercept\n",
        "Provides baseline\tExpected value of\n",
        "ùëå\n",
        "Y when all\n",
        "ùëã\n",
        "ùëñ\n",
        "=\n",
        "0\n",
        "X\n",
        "i\n",
        "‚Äã\n",
        " =0\n",
        "Adds context\tHelps interpret the regression line's position\n",
        "Practical meaning\tDepends on whether zero values are meaningful\n",
        "\n",
        "If you want, I can show how to interpret intercepts with examples\n",
        "\n",
        "\n",
        "| Aspect            | Role of Intercept                              |\n",
        "| ----------------- | ---------------------------------------------- |\n",
        "| Provides baseline | Expected value of $Y$ when all $X_i = 0$       |\n",
        "| Adds context      | Helps interpret the regression line's position |\n",
        "| Practical meaning | Depends on whether zero values are meaningful  |\n",
        "\n",
        "\n",
        "Q18.  What are the limitations of using R¬≤ as a sole measure of model performance\n",
        "\n",
        "Ans18. Limitations of Using R¬≤ as the Sole Measure of Model Performance\n",
        " While R¬≤ (coefficient of determination) indicates the proportion of variance explained by the model, relying on it alone has several drawbacks:\n",
        "\n",
        "Key Limitations:\n",
        "\n",
        "\n",
        "| Limitation                           | Explanation                                       |\n",
        "| ------------------------------------ | ------------------------------------------------- |\n",
        "| Overfitting Sensitivity              | R¬≤ can increase with irrelevant variables         |\n",
        "| No Information on Bias               | Does not show if model assumptions are violated   |\n",
        "| Poor for Non-Linear Models           | Not reliable outside linear regression            |\n",
        "| Does Not Reflect Prediction Accuracy | Doesn‚Äôt indicate how well model predicts new data |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPRP47sTifJE"
      }
    }
  ]
}