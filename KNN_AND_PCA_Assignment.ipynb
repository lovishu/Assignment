{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npv2uoy93vnQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is K-Nearest Neighbors (KNN) and how does it work\n",
        "\n",
        "Ans1. K-Nearest Neighbors (KNN) is a simple, intuitive, and widely used supervised machine learning algorithm used for classification and regression tasks.\n",
        "\n",
        "KNN is a non-parametric and instance-based learning algorithm that classifies or predicts a data point based on the labels of its nearest neighbors in the feature space.\n",
        "\n",
        "Key points:\n",
        "KNN is simple and effective, but can be slow with large datasets since it computes distances to all training points.\n",
        "\n",
        "The choice of K affects performance: a small K can be noisy, a large K can smooth out details.\n",
        "\n",
        "It works best when data is well-distributed and features are normalized/scaled.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q2.  What is the difference between KNN Classification and KNN Regression\n",
        "\n",
        "Ans2.The difference between KNN Classification and KNN Regression lies in the type of output they produce and how they aggregate the values of the nearest neighbors:\n",
        "\n",
        "üîπ KNN Classification\n",
        "Purpose: Predict a category/label (discrete output).\n",
        "\n",
        "Output: A class label (e.g., \"spam\" or \"not spam\", \"dog\" or \"cat\").\n",
        "\n",
        "üîπ KNN Regression\n",
        "Purpose: Predict a numerical value (continuous output).\n",
        "Output: A real number (e.g., price, temperature, height).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q3.  What is the role of the distance metric in KNN\n",
        "\n",
        "Ans3. The distance metric in K-Nearest Neighbors (KNN) is crucial because it determines how \"close\" or \"similar\" two data points are in the feature space.\n",
        "\n",
        "üîπ Role of Distance Metric:\n",
        "When a new data point is given, KNN uses the distance metric to compare it to all points in the training dataset.\n",
        "\n",
        "Based on these distances, it selects the K nearest neighbors.\n",
        "\n",
        "These neighbors are then used to make a prediction (via majority vote in classification or averaging in regression).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q4. What is the Curse of Dimensionality in KNN\n",
        "\n",
        "Ans4.The Curse of Dimensionality refers to problems that arise when working with data in high-dimensional spaces (i.e., when the number of features/variables is very large). In the context of K-Nearest Neighbors (KNN), it negatively affects the algorithm's performance and accuracy.\n",
        "\n",
        "KNN relies on distance calculations to find the nearest neighbors. But in high dimensions:\n",
        "\n",
        "Distances become less meaningful:\n",
        "\n",
        "Sparsity of data:\n",
        "\n",
        "Increased computation:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q5. How can we choose the best value of K in KNN\n",
        "\n",
        "Ans5. Choosing the right value of K (the number of nearest neighbors) is critical for the performance of a KNN model. A poor choice can lead to overfitting or underfitting.\n",
        "\n",
        "\n",
        "| Value of K          | Behavior                | Effect                          |\n",
        "| ------------------- | ----------------------- | ------------------------------- |\n",
        "| Small K (e.g., K=1) | Very sensitive to noise | High variance ‚Üí **overfitting** |\n",
        "| Large K             | Smoother predictions    | High bias ‚Üí **underfitting**    |\n",
        "\n",
        "\n",
        "\n",
        "Q6. What are KD Tree and Ball Tree in KNN\n",
        "\n",
        "Ans6.üîπ 1. KD Tree (K-Dimensional Tree)\n",
        "KD Tree and Ball Tree are data structures used to speed up the process of finding nearest neighbors in K-Nearest Neighbors (KNN), especially for large datasets.\n",
        "\n",
        "KNN is a lazy and instance-based algorithm. Without optimization, it must compute the distance between the test point and every point in the training set ‚Äî which is slow. KD Tree and Ball Tree help reduce this computational cost.\n",
        "\n",
        "üîπ 1. KD Tree (K-Dimensional Tree)\n",
        "\n",
        "üîπ 2. Ball Tree\n",
        "\n",
        "\n",
        "\n",
        "Q6.  When should you use KD Tree vs. Ball Tree\n",
        "\n",
        "Ans6.The choice between KD Tree and Ball Tree depends mainly on the dimensionality of your data and its distribution.\n",
        "\n",
        "| Condition                         | Reason                                                                   |\n",
        "| --------------------------------- | ------------------------------------------------------------------------ |\n",
        "| **Low-dimensional data** (‚â§ 20)   | KD Tree performs fast axis-aligned splits efficiently in low dimensions. |\n",
        "| Data is **uniformly distributed** | KD Tree divides space cleanly when data is evenly spread.                |\n",
        "| You need **simple, fast queries** | KD Tree has lower overhead in simple spaces.                             |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q7.  What are the disadvantages of KNN\n",
        "\n",
        "Ans7.\n",
        " While KNN is simple and effective for many problems, it also has several limitations that can affect its performance and scalability.\n",
        "\n",
        "üîπ 1. Slow Prediction Time\n",
        "\n",
        "üîπ 2. Sensitive to Irrelevant or Redundant Features\n",
        "\n",
        "üîπ 3. Curse of Dimensionality\n",
        "\n",
        "üîπ 4. Memory Intensive\n",
        "\n",
        "\n",
        "\n",
        "Q8. How does feature scaling affect KNN\n",
        "\n",
        " Ans8. Feature scaling is critical for the performance of K-Nearest Neighbors (KNN) because KNN is a distance-based algorithm.\n",
        "\n",
        "\n",
        "üîπ Why is scaling important?\n",
        "KNN uses distance metrics like Euclidean or Manhattan to compute the closeness between data points:\n",
        "\n",
        "\n",
        "\n",
        "| Method             | Description                              | Use Case                             |\n",
        "| ------------------ | ---------------------------------------- | ------------------------------------ |\n",
        "| **StandardScaler** | Mean = 0, Std Dev = 1                    | Common for normally distributed data |\n",
        "| **MinMaxScaler**   | Scales features to \\[0, 1] range         | Best for bounded feature ranges      |\n",
        "| **RobustScaler**   | Uses median and IQR (robust to outliers) | Good when data has outliers          |\n",
        "\n",
        "\n",
        "\n",
        "Q10.  What is PCA (Principal Component Analysis)\n",
        "\n",
        "Ans10. Principal Component Analysis (PCA) is a dimensionality reduction technique used to simplify complex datasets by transforming them into a smaller number of uncorrelated variables called principal components, while preserving as much variance (information) as possible.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q11.  How does PCA work\n",
        "\n",
        "Ans11. Principal Component Analysis (PCA) works by finding new axes (directions) ‚Äî called principal components ‚Äî that best capture the maximum variance in the data. It then projects the data onto these axes to reduce the number of dimensions while retaining as much information as possible.\n",
        "\n",
        "‚úÖ 1. Standardize the Data\n",
        "\n",
        "‚úÖ 2. Compute the Covariance Matrix\n",
        "\n",
        "‚úÖ 3. Compute Eigenvectors and Eigenvalues\n",
        "\\\n",
        "‚úÖ 4. Sort Eigenvalues and Select Top k Components\n",
        "\n",
        "\n",
        "\n",
        "Q12.  What is the geometric intuition behind PCA\n",
        "\n",
        "Ans12. Geometric Intuition of PCA:\n",
        "\n",
        "Imagine you have a cloud of data points scattered in a high-dimensional space. The goal of PCA is to find new axes (directions) along which the data varies the most, and then project the data onto those axes to simplify it while preserving the main structure.\n",
        "\n",
        "Q13.  What is the difference between Feature Selection and Feature Extraction\n",
        "\n",
        "Ans13. | Aspect         | Feature Selection                                                       | Feature Extraction                                                  |\n",
        "| -------------- | ----------------------------------------------------------------------- | ------------------------------------------------------------------- |\n",
        "| **Definition** | Selecting a **subset** of the original features based on some criteria. | Creating **new features** by transforming the original features.    |\n",
        "| **Output**     | Subset of original features (unchanged features).                       | New features (combinations or projections of original features).    |\n",
        "| **Goal**       | Keep the most relevant original features.                               | Create compact, informative features summarizing the original data. |\n",
        "| **Examples**   | - Filter methods (e.g., correlation, Chi-square)                        |                                                                     |\n",
        "\n",
        "Q14.  What are Eigenvalues and Eigenvectors in PCA\n",
        "\n",
        "Ans14. In PCA (Principal Component Analysis), eigenvalues and eigenvectors come from the covariance matrix of the data and are fundamental to identifying the directions (principal components) that capture the most variance.\n",
        "\n",
        "üîπ What are Eigenvectors?\n",
        "An eigenvector is a direction (a vector) in the feature space.\n",
        "\n",
        "üîπ What are Eigenvalues?\n",
        "An eigenvalue is a scalar associated with each eigenvector.\n",
        "\n",
        "It measures the amount of variance in the data along its corresponding eigenvector (principal component).\n",
        "\n",
        "üîπ Role in PCA:\n",
        "PCA computes eigenvectors and eigenvalues of the covariance matrix of the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q15.  How do you decide the number of components to keep in PCA\n",
        "\n",
        "Ans15. To decide how many principal components to keep in PCA, you typically use one or more of these approaches:\n",
        "\n",
        "Explained Variance Threshold:\n",
        "\n",
        "Scree Plot (Elbow Method):\n",
        "Plot the cumulative explained variance or eigenvalues versus the number of components. Pick the number at the ‚Äúelbow‚Äù point where additional components add little extra variance.\n",
        "\n",
        "Keep components with eigenvalues greater than 1 (mostly in factor analysis contexts).\n",
        "\n",
        "Cross-Validation / Downstream Performance:\n",
        "Experiment with different numbers of components and choose the number that gives the best performance in your specific task (e.g., classification accuracy).\n",
        "\n",
        "\n",
        "Q16. Can PCA be used for classification\n",
        "\n",
        "\n",
        "Ans16. PCA itself is not a classification algorithm, but it can be very useful as a preprocessing step for classification tasks.\n",
        "\n",
        "\n",
        "How PCA helps in classification:\n",
        "Dimensionality Reduction:\n",
        "Noise Reduction:\n",
        "Improved Model Performance:\n",
        "\n",
        "Many classifiers (like KNN, SVM, logistic regression) perform better or faster with fewer, uncorrelated features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q17. What are the limitations of PCA\n",
        "\n",
        "Ans17.\n",
        "PCA assumes that the data‚Äôs structure can be captured by linear combinations of features. It fails to capture non-linear relationships in the data.\n",
        "\n",
        "Loss of Interpretability\n",
        "Principal components are linear combinations of original features and can be hard to interpret, especially in high dimensions.\n",
        "\n",
        "Variance Does Not Equal Importance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q18.  How do KNN and PCA complement each other\n",
        "\n",
        "Ans18. 1. PCA reduces dimensionality for KNN\n",
        "\n",
        "\n",
        "KNN performance degrades with high-dimensional data (curse of dimensionality).\n",
        "\n",
        "PCA reduces the number of features by projecting data onto fewer principal components while preserving most of the variance.\n",
        "\n",
        "2. PCA speeds up KNN\n",
        "KNN computes distances between points, which gets slower with more features.\n",
        "\n",
        "\n",
        "\n",
        "Q19.  How does KNN handle missing values in a dataset\n",
        "\n",
        "Ans19. KNN itself does not inherently handle missing values. You need to preprocess the data before applying KNN. Here are common strategies:\n",
        "\n",
        "1. Imputation Before KNN\n",
        "Mean/Median/Mode Imputation:\n",
        "\n",
        "KNN Imputation:\n",
        "\n",
        "Use KNN itself to impute missing values by finding nearest neighbors based on other features and averaging their values.\n",
        "\n",
        "2. Remove Samples or Features\n",
        "\n",
        "\n",
        "3. Distance Calculation Adjustments\n",
        "Some advanced KNN implementations adjust distance calculations to ignore missing values or estimate distances based on available features, but this is not standard.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q20.  What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
        "\n",
        "Ans20.\n",
        " | Aspect                   | PCA (Principal Component Analysis)                      | LDA (Linear Discriminant Analysis)                                                               |\n",
        "| ------------------------ | ------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |\n",
        "| **Purpose**              | Unsupervised dimensionality reduction                   | Supervised dimensionality reduction and classification                                           |\n",
        "| **Goal**                 | Maximize variance in data (capture most information)    | Maximize class separability (maximize between-class variance and minimize within-class variance) |\n",
        "| **Use of Labels**        | Does **not** use class labels                           | Uses class labels                                                                                |\n",
        "| **Components**           | Principal components are directions of maximum variance | Discriminant components maximize class separability                                              |\n",
        "| **Number of Components** | Up to number of original features                       | Up to (number of classes - 1) components                                                         |\n",
        "| **Assumptions**          | No assumptions about data distribution                  | Assumes normally distributed classes with equal covariance matrices                              |\n",
        "| **Typical Use Cases**    | Data visualization, noise reduction, preprocessing      | Classification, supervised feature extraction                                                    |\n",
        "| **Feature Extraction**   | Finds axes capturing most overall data variance         | Finds axes that best separate classes                                                            |\n",
        "| **Interpretability**     | Components represent major variance directions          | Components represent directions maximizing class separation                                      |\n",
        "\n",
        "\n",
        "\n",
        " Practical\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q21.  Train a KNN Classifier on the Iris dataset and print model accuracy\n",
        "\n",
        "Ans21. from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize KNN with k=5 (default)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"KNN Classifier Accuracy on Iris dataset: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q22.  Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "\n",
        "Ans22.import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize KNN Regressor with k=5\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"KNN Regressor Mean Squared Error on synthetic data: {mse:.2f}\")\n",
        "\n",
        "\n",
        "Q23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy\n",
        "\n",
        "\n",
        "Ans23. from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize KNN with Euclidean distance (default)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# Initialize KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(f\"Accuracy with Euclidean distance: {accuracy_euclidean:.2f}\")\n",
        "print(f\"Accura\n",
        "\n",
        "\n",
        "\n",
        "Q24.  Train a KNN Classifier with different values of K and visualize decision boundarie\n",
        "\n",
        "Ans24.import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset and select first two features for visualization\n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # Only first two features (sepal length, sepal width)\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_decision_boundary(clf, X, y, ax, title):\n",
        "    h = 0.02  # step size in mesh\n",
        "\n",
        "\n",
        "Q25. Apply Feature Scaling before training a KNN model and compare results with unscaled data\n",
        "\n",
        "Ans25.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Without scaling ---\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(_\n",
        "\n",
        "\n",
        "\n",
        "Q26. Train a PCA model on synthetic data and print the explained variance ratio for each component\n",
        "\n",
        "\n",
        "Ans26.\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Generate synthetic data (e.g., classification data with 10 features)\n",
        "X, _ = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Print explained variance ratio for each component\n",
        "explained_variance_rati\n",
        "\n",
        "\n",
        "Q27.  Apply PCA before training a KNN Classifier and compare accuracy with and without PCA\n",
        "\n",
        "\n",
        "Ans27.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features (important for both PCA and KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- KNN without PCA ---\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "accuracy_without_pca = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# --- Apply PCA ---\n",
        "pca = PCA(n_components=2)  # Reduce to 2 components for example\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# --- KNN with PCA ---\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(_\n",
        "\n",
        "\n",
        "Q28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV\n",
        "\n",
        "\n",
        "Ans28. from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define parameter grid to search\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=knn, p_\n",
        "\n",
        "\n",
        "\n",
        "Q29.  Train a KNN Classifier and check the number of misclassified samples\n",
        "\n",
        "Ans29.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Calculate misclassified samples\n",
        "misclassified = (y_test != y_pred).sum()\n",
        "print(f\"Number of misclassified samples: {misclassified}\")\n",
        "\n",
        "Q30.  Train a PCA model and visualize the cumulative explained variance.\n",
        "\n",
        "Ans30.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Generate synthetic data with 10 features\n",
        "X, _ = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1),_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "He3c39mD3z6C"
      }
    }
  ]
}