{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl5qzAydWUJg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.  What is unsupervised learning in the context of machine learning\n",
        "\n",
        "Ans1. Unsupervised learning is a type of machine learning where the model is trained on data without labeled outputs. The algorithm tries to learn the underlying structure or distribution in the data to discover patterns, groupings, or features.\n",
        "\n",
        "Key Characteristics:\n",
        "No labeled data: The input data lacks predefined categories or outcomes.\n",
        "\n",
        "Goal: Find hidden patterns, structure, or relationships in the data.\n",
        "\n",
        "In summary, unsupervised learning is useful when you want to explore data and uncover hidden patterns without prior knowledge of outcomes.\n",
        "\n",
        "\n",
        "Q2.  How does K-Means clustering algorithm work\n",
        "Ans2. K-Means clustering is a popular unsupervised learning algorithm used to partition data into K distinct clusters based on similarity.\n",
        "\n",
        "How K-Means Works: Step-by-Step\n",
        "\n",
        "Choose the number of clusters (K)\n",
        "\n",
        "You decide how many clusters (K) you want to divide your data into.\n",
        "\n",
        "Initialize centroids\n",
        "\n",
        "Randomly select K data points as the initial centroids (the center of each cluster).\n",
        "\n",
        "Assign points to the nearest centroid\n",
        "\n",
        "Suppose you have data points of customer purchases (e.g., spending on food and clothing). Using K-Means:\n",
        "\n",
        "The algorithm groups customers into 3 clusters,\n",
        "\n",
        "Each cluster represents a group with similar spending habits.\n",
        "\n",
        "\n",
        "\n",
        "Q3. Explain the concept of a dendrogram in hierarchical clustering\n",
        "\n",
        "Ans3. A dendrogram is a tree-like diagram used to represent the arrangement of clusters in hierarchical clustering. It visually shows how data points are merged or split across different levels of similarity.\n",
        "\n",
        "üîç Purpose of a Dendrogram\n",
        "To illustrate the hierarchy of clusters formed at different distances (or similarities).\n",
        "\n",
        "To help decide the optimal number of clusters by cutting the dendrogram at a chosen level.\n",
        "\n",
        "üß± How to Read a Dendrogram\n",
        "\n",
        "\n",
        "Leaves (bottom nodes): Represent individual data points.\n",
        "\n",
        "Branches: Show how points or clusters are combined.\n",
        "\n",
        "Height of branches: Indicates the distance (or dissimilarity) at which clusters were merged.\n",
        "\n",
        "Cutting the dendrogram: A horizontal line at a particular height cuts the tree into separate clusters.\n",
        "\n",
        "\n",
        "\n",
        "Q4. What is the main difference between K-Means and Hierarchical Clustering\n",
        "\n",
        "\n",
        "Ans4. The main difference between K-Means and Hierarchical Clustering lies in how clusters are formed and structured:\n",
        "\n",
        "| Feature                | **K-Means Clustering**                                | **Hierarchical Clustering**                             |\n",
        "| ---------------------- | ----------------------------------------------------- | ------------------------------------------------------- |\n",
        "| **Cluster Structure**  | Flat (non-hierarchical)                               | Hierarchical (nested clusters shown via dendrogram)     |\n",
        "| **Need to specify K?** | Yes, number of clusters (K) must be chosen in advance | No, hierarchy is built without specifying cluster count |\n",
        "| **Approach**           | Partitional: Divides data into K separate clusters    | Agglomerative (bottom-up) or divisive (top-down)        |\n",
        "| **Scalability**        | Efficient for large datasets                          | Slower and more memory-intensive with large data        |\n",
        "| **Cluster Shape**      | Works best with spherical, evenly sized clusters      | Can capture complex shapes and nested structures        |\n",
        "| **Reproducibility**    | May give different results (random init of centroids) | Deterministic (same output every time)                  |\n",
        "| **Output**             | A fixed set of K clusters                             | A dendrogram showing all possible clusterings           |\n",
        "\n",
        "\n",
        "\n",
        "Q5.  What are the advantages of DBSCAN over K-Means\n",
        "\n",
        "Ans5. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) offers several key advantages over K-Means, particularly in handling complex data distributions and noise.\n",
        "\n",
        "| Feature/Capability                         | **DBSCAN**                                               | **K-Means**                                          |\n",
        "| ------------------------------------------ | -------------------------------------------------------- | ---------------------------------------------------- |\n",
        "| **No need to specify number of clusters**  | ‚úÖ Automatically finds the number of clusters             | ‚ùå Requires user to pre-define **K**                  |\n",
        "| **Handles arbitrary cluster shapes**       | ‚úÖ Can detect non-spherical, irregularly shaped clusters  | ‚ùå Assumes spherical clusters                         |\n",
        "| **Robust to noise and outliers**           | ‚úÖ Can identify and ignore outliers as **noise**          | ‚ùå Sensitive to outliers, which can distort centroids |\n",
        "| **Works with clusters of varying density** | ‚úÖ (with tuning of parameters)                            | ‚ùå Assumes clusters are of similar size/density       |\n",
        "| **Deterministic**                          | ‚úÖ Always gives the same result (if parameters are fixed) | ‚ùå Result depends on random centroid initialization   |\n",
        "\n",
        "\n",
        "\n",
        "Q6.  When would you use Silhouette Score in clustering\n",
        "\n",
        "Ans6. Hierarchical Clustering, while useful for exploring data structure, has several important limitations:\n",
        "\n",
        "\n",
        "‚ö†Ô∏è Limitations of Hierarchical Clustering\n",
        "Scalability Issues\n",
        "\n",
        "No Reassignment\n",
        "\n",
        "Choice of Linkage and Distance Metrics\n",
        "\n",
        "\n",
        "\n",
        "Q8. Why is feature scaling important in clustering algorithms like K-Means\n",
        "\n",
        "Ans7. Feature scaling is crucial in clustering algorithms like K-Means because these algorithms rely on distance calculations (usually Euclidean distance) to group data points.\n",
        "\n",
        "üìè Why Feature Scaling Matters in K-Means\n",
        "\n",
        " K-Means Uses Distance-Based Similarity\n",
        "\n",
        "Unscaled Features Lead to Skewed Clusters\n",
        "\n",
        "\n",
        "Distorts Centroids and Clustering Results\n",
        "\n",
        "üß† Summary\n",
        "Without feature scaling, K-Means can produce misleading clusters due to the dominance of features with larger numeric ranges. Scaling ensures fair contribution of each feature in forming meaningful clusters.\n",
        "\n",
        "\n",
        "\n",
        "Q9. How does DBSCAN identify noise points\n",
        "\n",
        "Ans9. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) identifies noise points by analyzing the density of data points in a given region. Noise points are those that do not belong to any cluster based on density criteria.\n",
        "\n",
        "\n",
        "üîç Key Concepts in DBSCAN\n",
        "\n",
        "minPts: Minimum number of points required within the eps radius to form a dense region.\n",
        "\n",
        "Core Point: Has at least minPts points (including itself) within its eps-radius neighborhood.\n",
        "\n",
        "\n",
        "Border Point: Has fewer than minPts, but lies within the eps neighborhood of a core point.\n",
        "\n",
        "\n",
        "Q10.  Define inertia in the context of K-Means\n",
        "\n",
        "Ans10. In the context of K-Means clustering, inertia is a measure of how internally coherent the clusters are. It quantifies the sum of squared distances between each data point and the centroid of the cluster it belongs to.\n",
        "\n",
        "üß† Summary\n",
        "\n",
        "Inertia measures the compactness of clusters in K-Means. While a useful metric, it should be used in combination with other validation methods (e.g., silhouette score), especially since inertia always decreases with more clusters.\n",
        "\n",
        "\n",
        "\n",
        "Q11.  What is the elbow method in K-Means clustering\n",
        "\n",
        "\n",
        "Ans11.\n",
        "\n",
        "The Elbow Method is a technique used in K-Means clustering to determine the optimal number of clusters (K) by analyzing how inertia (within-cluster sum of squares) changes as K increases.\n",
        "\n",
        "üîç Steps of the Elbow Method\n",
        "\n",
        "\n",
        "üìâ What the Plot Shows\n",
        "\n",
        "\n",
        "X-axis: Number of clusters (K)\n",
        "\n",
        "Y-axis: Inertia (sum of squared distances to centroids)\n",
        "\n",
        "The \"elbow\" resembles a sharp bend in the curve.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q12. Describe the concept of \"density\" in DBSCAN\n",
        "\n",
        "Ans12.\n",
        "In DBSCAN, density refers to the concentration of data points in a given region of the feature space. It‚Äôs the fundamental concept that allows DBSCAN to identify clusters based on areas where points are densely packed together, separated by regions of lower point density.\n",
        "\n",
        "\n",
        "üîë Key Concepts of Density in DBSCAN\n",
        "\n",
        "Neighborhood Radius (eps):\n",
        "\n",
        "For each point, DBSCAN considers a neighborhood defined by a radius eps around that point.\n",
        "\n",
        "Core Points:\n",
        "\n",
        "\n",
        "\n",
        "Q13.  Can hierarchical clustering be used on categorical data\n",
        "\n",
        "Ans13. Yes, hierarchical clustering can be used on categorical data, but it requires careful handling because standard hierarchical clustering typically relies on distance metrics like Euclidean distance, which are designed for numerical data.\n",
        "\n",
        "\n",
        "How hierarchical clustering can be applied to categorical data:\n",
        "\n",
        "Use appropriate distance/similarity measures for categorical data:\n",
        "\n",
        "Convert categorical data to numeric format (optional):\n",
        "\n",
        "Use specialized hierarchical clustering algorithms:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q14.  What does a negative Silhouette Score indicate\n",
        "\n",
        "\n",
        "Ans14.\n",
        "A negative Silhouette Score indicates that a data point is likely assigned to the wrong cluster ‚Äî it is closer to points in another cluster than to points in its own cluster.\n",
        "\n",
        "üìê Silhouette Score Basics\n",
        "The Silhouette Score for a single point is defined as:\n",
        "\n",
        "b‚àía\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q15.  Explain the term \"linkage criteria\" in hierarchical clustering\n",
        "\n",
        "\n",
        "Ans15.In hierarchical clustering, the term \"linkage criteria\" refers to the method used to measure the distance between clusters when combining them during the clustering process.\n",
        "\n",
        "It determines how the distance between two clusters is computed, which directly affects the structure of the dendrogram and the final clusters formed.\n",
        "\n",
        "| Linkage Type         | Description                                                              | Behavior                                    |\n",
        "| -------------------- | ------------------------------------------------------------------------ | ------------------------------------------- |\n",
        "| **Single Linkage**   | Distance between the **closest pair** of points (one from each cluster)  | Tends to form **long, chain-like clusters** |\n",
        "| **Complete Linkage** | Distance between the **farthest pair** of points (one from each cluster) | Results in **compact, spherical clusters**  |\n",
        "| **Average Linkage**  | Average distance between **all pairs** of points (one from each cluster) | Balances compactness and flexibility        |\n",
        "| **Centroid Linkage** | Distance between the **centroids (means)** of the clusters               | Can cause **inversions** in dendrogram      |\n",
        "| **Ward‚Äôs Method**    | Minimizes the **total within-cluster variance** when merging clusters    | Tends to create **equal-sized clusters**    |\n",
        "\n",
        "\n",
        "\n",
        "Q16.  Why might K-Means clustering perform poorly on data with varying cluster sizes or densities\n",
        "\n",
        "Ans16. K-Means clustering can perform poorly on data with varying cluster sizes or densities because it makes strong assumptions that often do not hold in such cases.\n",
        "\n",
        "‚ùå Why K-Means Struggles\n",
        "Assumes Equal Cluster Size\n",
        "\n",
        "Sensitive to Density Variations\n",
        "\n",
        "Affected by Outliers\n",
        "\n",
        "Assumes Spherical Clusters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q17.  What are the core parameters in DBSCAN, and how do they influence clustering\n",
        "\n",
        "\n",
        "Ans17. The two core parameters in DBSCAN (Density-Based Spatial Clustering of Applications with Noise) are:\n",
        "\n",
        "1. eps (epsilon) ‚Äî Neighborhood Radius\n",
        "\n",
        "Defines the radius of the neighborhood around a point.\n",
        "\n",
        "Determines how close points need to be to be considered neighbors.\n",
        "\n",
        "2. minPts ‚Äî Minimum Points\n",
        "\n",
        "\n",
        "Minimum number of points (including the point itself) required to form a dense region (i.e., a core point).\n",
        "\n",
        "\n",
        "Q18.  How does K-Means++ improve upon standard K-Means initialization\n",
        "\n",
        "Ans18. K-Means++ improves upon standard K-Means by providing a smarter way to initialize the centroids, which helps the algorithm converge faster and avoid poor clustering results due to bad initial placement.\n",
        "\n",
        "üîç Standard K-Means Initialization:\n",
        "\n",
        "Chooses K initial centroids randomly from the data.\n",
        "\n",
        "‚úÖ K-Means++ Initialization Steps:\n",
        "\n",
        "\n",
        "\n",
        "Randomly choose the first centroid from the data points.\n",
        "\n",
        "For each remaining data point, compute the squared distance to the nearest already chosen centroid.\n",
        "\n",
        "Select the next centroid with a probability proportional to the squared distance ‚Äî i.e., points farther from existing centroids have a higher chance of being chosen.\n",
        "\n",
        "Repeat Step 3 until K centroids are chosen.\n",
        "\n",
        "\n",
        "\n",
        "Q19. What is agglomerative clustering\n",
        "\n",
        "\n",
        "Ans19. Agglomerative clustering is a type of hierarchical clustering that builds clusters in a bottom-up manner.\n",
        "\n",
        "üîß How Agglomerative Clustering Works:\n",
        "Start with each data point as its own cluster.\n",
        "\n",
        "Iteratively merge the two closest clusters based on a linkage criterion (e.g., single, complete, average).\n",
        "\n",
        "üß† Summary:\n",
        "Agglomerative clustering is a hierarchical method that repeatedly merges the most similar clusters until all points are grouped or a threshold is met. It‚Äôs simple, intuitive, and useful for exploring data structure through dendrograms.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q20.  What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "\n",
        "Ans20. The Silhouette Score is generally a better clustering evaluation metric than inertia alone because it balances both cohesion (how similar points are within a cluster) and separation (how distinct clusters are from each other).\n",
        "\n",
        "\n",
        "| Aspect                 | Inertia                    | Silhouette Score                       |\n",
        "| ---------------------- | -------------------------- | -------------------------------------- |\n",
        "| **Cluster Cohesion**   | ‚úîÔ∏è Measures it             | ‚úîÔ∏è Measures it                         |\n",
        "| **Cluster Separation** | ‚ùå Ignores it               | ‚úîÔ∏è Includes it                         |\n",
        "| **Optimal K Guidance** | ‚ùå Always decreases with K  | ‚úîÔ∏è Peaks at optimal K                  |\n",
        "| **Model Comparison**   | Difficult with different K | Easier due to standardized score range |\n",
        "\n",
        "\n",
        "\n",
        "Q21.  Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot\n",
        "\n",
        "Ans21.\n",
        "\n",
        " Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a\n",
        " scatter plot\n",
        "\n",
        " Here is the scatter plot showing the results of K-Means clustering on synthetic data with 4 centers. Each color represents a different cluster, and the red \"X\" markers denote the cluster centroids found by the algorithm. Let me know if you‚Äôd like to explore silhouette scores or try a different clustering method!\n",
        "\n",
        "\n",
        "Q22.  Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels\n",
        "\n",
        " Ans22.\n",
        "\n",
        " The first 10 predicted labels from Agglomerative Clustering on the Iris dataset are:\n",
        "\n",
        "[1 1 1 1 1 1 1 1 1 1]\n",
        "\n",
        "All 10 data points were assigned to cluster label 1. Let me know if you'd like a full cluster distribution or a visualization of the clustering.\n",
        "\n",
        "\n",
        "\n",
        "Q23.  Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot\n",
        "\n",
        "Ans23. The plot above shows the result of applying DBSCAN on the two-moon dataset:\n",
        "\n",
        "Colored points represent data assigned to clusters.\n",
        "\n",
        "Black 'x' markers indicate outliers (noise points) that DBSCAN could not assign to any cluster based on the density criteria.\n",
        "\n",
        "\n",
        "\n",
        "Q24.  Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster\n",
        "\n",
        "Ans24. Here's a precise and correct Python code snippet to load the Wine dataset, standardize its features, apply K-Means clustering, and print the size of each cluster:\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)  # Wine dataset has 3 classes\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Print size of each cluster\n",
        "(unique, counts) = np.unique(kmeans.labels_, return_counts=True)\n",
        "for cluster_id, size in zip(unique, counts):\n",
        "    print(f\"Cluster {cluster_id}: {size} samples\")\n",
        "\n",
        "\n",
        "Q25.  Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result\n",
        "\n",
        "\n",
        "Ans25. import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data\n",
        "X, _ = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "unique_labels = set(labels)\n",
        "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
        "\n",
        "for k, col in zip(unique_labels, colors):\n",
        "    if k == -1:\n",
        "        # Black used for noise.\n",
        "        col = [0, 0, 0, 1]\n",
        "    class_member_mask = (labels == k)\n",
        "    xy = X[class_member_mask]\n",
        "    plt.scatter(xy[:, 0], xy[:, 1], s=50, color=col, label=f'Cluster {k}' if k != -1 else 'Noise')\n",
        "\n",
        "plt.title('DBSCAN clustering on make_circles data')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q26.  Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centorids\n",
        "\n",
        "Ans26. from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# Apply MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means with 2 clusters\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Output cluster centroids\n",
        "print(\"Cluster centroids (in scaled feature space):\")\n",
        "print(kmeans.cluster_centers_)\n",
        "\n",
        "\n",
        "Q27.  Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN\n",
        "\n",
        "Ans27. import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data with varying cluster std deviations\n",
        "X, _ = make_blobs(n_samples=500, centers=3, cluster_std=[0.5, 1.5, 0.3], random_state=42)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.7, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "unique_labels = set(labels)\n",
        "colors = [plt.cm.Set1(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
        "\n",
        "for k, col in zip(unique_labels, colors):\n",
        "    if k == -1:\n",
        "        col = [0, 0, 0, 1]  # Black color for noise\n",
        "    class_member_mask = (labels == k)\n",
        "    xy = X[class_member_mask]\n",
        "    plt.scatter(xy[:, 0], xy[:, 1], s=50, color=col, label=f'Cluster {k}' if k != -1 else 'Noise')\n",
        "\n",
        "plt.title('DBSCAN clustering on make_blobs data with varying std deviations')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q28.  Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means\n",
        "\n",
        "\n",
        "Ans28. import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce to 2D using PCA\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering (10 clusters for 10 digits)\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# Plot clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=30)\n",
        "plt.title('K-Means Clusters on Digits Dataset (PCA reduced)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.colorbar(scatter, ticks=range(10), label='Cluster Label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q29.  Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart\n",
        "\n",
        "\n",
        "Ans29.import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "sil_scores = []\n",
        "k_values = range(2, 6)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    sil_scores.append(score)\n",
        "\n",
        "# Plotting silhouette scores as a bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(k_values, sil_scores, color='skyblue')\n",
        "plt.xticks(k_values)\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Scores for K-Means Clustering')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Q30.  Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage\n",
        "\n",
        "\n",
        "Ans30. import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Perform hierarchical clustering with average linkage\n",
        "Z = linkage(X, method='average')\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(Z, labels=iris.target, leaf_rotation=90)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Average Linkage)')\n",
        "plt.xlabel('Sample Index or (Cluster Size)')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A_NkglydWVrZ"
      }
    }
  ]
}